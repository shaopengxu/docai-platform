"""
DocAI Platform - RAG è´¨é‡è¯„ä¼°ä¸æ¶ˆèå®éªŒ (Ablation Study)
å¯¹æ¯”æœ¬ç³»ç»Ÿçš„å¢å¼º RAG ä¸ç®€å• RAG åŸºçº¿çš„æ•ˆæœå·®å¼‚ã€‚

ç”¨æ³•:
    # 1. å…ˆç¡®ä¿åŸºç¡€æœåŠ¡è¿è¡Œä¸”è‡³å°‘ä¸Šä¼ äº†ä¸€äº›æ–‡æ¡£
    # 2. è¿è¡Œå®Œæ•´è¯„ä¼°
    python -m scripts.eval_rag_quality

    # 3. åªè¿è¡Œæ£€ç´¢å¯¹æ¯”
    python -m scripts.eval_rag_quality --retrieval-only

    # 4. ä½¿ç”¨è‡ªå®šä¹‰æµ‹è¯•é›†
    python -m scripts.eval_rag_quality --test-file tests/my_questions.json
"""

from __future__ import annotations

import argparse
import asyncio
import json
import time
from dataclasses import dataclass, field
from pathlib import Path

import structlog

# â”€â”€ å¯¼å…¥ç³»ç»Ÿæ¨¡å— â”€â”€
from app.core.infrastructure import get_db_session
from app.core.llm_client import llm, llm_light
from app.core.models import RetrievedChunk
from app.generation.answer import generate_answer
from app.retrieval.hybrid_search import (
    _bm25_search,
    _rerank,
    _rrf_fusion,
    _vector_search,
    hybrid_search,
)
from config.settings import settings

logger = structlog.get_logger()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®ç»“æ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dataclass
class TestCase:
    """ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹"""
    question: str
    expected_answer: str = ""          # æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼Œå¦‚æœ‰åˆ™è¯„ä¼°å‡†ç¡®æ€§ï¼‰
    expected_doc_id: str = ""          # é¢„æœŸå‘½ä¸­çš„æ–‡æ¡£ ID
    expected_section: str = ""         # é¢„æœŸå‘½ä¸­çš„ç« èŠ‚è·¯å¾„
    category: str = "factual"          # factual / summary / comparison / version_diff


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    method: str                        # æ£€ç´¢æ–¹æ³•åç§°
    chunks: list[RetrievedChunk]
    latency_ms: int
    top_scores: list[float] = field(default_factory=list)

    @property
    def hit_count(self) -> int:
        return len(self.chunks)

    @property
    def avg_score(self) -> float:
        return sum(self.top_scores) / len(self.top_scores) if self.top_scores else 0

    @property
    def max_score(self) -> float:
        return max(self.top_scores) if self.top_scores else 0


@dataclass
class GenerationResult:
    """ç”Ÿæˆç»“æœ"""
    method: str
    answer: str
    confidence: float
    latency_ms: int
    citation_count: int


@dataclass
class EvalReport:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è¯„ä¼°æŠ¥å‘Š"""
    question: str
    category: str
    retrieval_results: dict[str, RetrievalResult] = field(default_factory=dict)
    generation_results: dict[str, GenerationResult] = field(default_factory=dict)
    relevance_scores: dict[str, float] = field(default_factory=dict)  # LLM è¯„åˆ¤çš„ç›¸å…³æ€§


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ£€ç´¢æ–¹æ³•: ä»æœ€ç®€å•åˆ°æœ€å®Œæ•´ï¼Œé€å±‚å åŠ 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def retrieval_vector_only(query: str, top_k: int = 5) -> list[RetrievedChunk]:
    """æ–¹æ³• A: ä»…å‘é‡æ£€ç´¢ (æœ€ç®€å•çš„åŸºçº¿)"""
    results = await _vector_search(query)
    return sorted(results, key=lambda x: x.score, reverse=True)[:top_k]


async def retrieval_bm25_only(query: str, top_k: int = 5) -> list[RetrievedChunk]:
    """æ–¹æ³• B: ä»… BM25 å…³é”®è¯æ£€ç´¢"""
    results = await _bm25_search(query)
    return sorted(results, key=lambda x: x.score, reverse=True)[:top_k]


async def retrieval_hybrid_no_rerank(query: str, top_k: int = 5) -> list[RetrievedChunk]:
    """æ–¹æ³• C: æ··åˆæ£€ç´¢ (å‘é‡ + BM25 + RRF) ä½†ä¸ç”¨ Reranker"""
    vector_results = await _vector_search(query)
    bm25_results = await _bm25_search(query)
    fused = _rrf_fusion(vector_results, bm25_results)
    return sorted(fused, key=lambda x: x.score, reverse=True)[:top_k]


async def retrieval_hybrid_with_rerank(query: str, top_k: int = 5) -> list[RetrievedChunk]:
    """æ–¹æ³• D: æ··åˆæ£€ç´¢ + Reranker (ä¸å«ä¸Šä¸‹æ–‡æ‰©å±•)"""
    vector_results = await _vector_search(query)
    bm25_results = await _bm25_search(query)
    fused = _rrf_fusion(vector_results, bm25_results)
    reranked = _rerank(query, fused)  # åŒæ­¥å‡½æ•°
    return reranked[:top_k]


async def retrieval_full_pipeline(query: str, top_k: int = 5) -> list[RetrievedChunk]:
    """æ–¹æ³• E: å®Œæ•´ Pipeline (å‘é‡+BM25+RRF+Reranker+ä¸Šä¸‹æ–‡æ‰©å±•)"""
    return await hybrid_search(query=query, top_k=top_k, use_reranker=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç”Ÿæˆæ–¹æ³•: å¯¹æ¯”ä¸åŒæ£€ç´¢åŸºç¡€ä¸Šçš„ç­”æ¡ˆè´¨é‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def generate_with_chunks(
    question: str, chunks: list[RetrievedChunk], method_name: str
) -> GenerationResult:
    """åŸºäºç»™å®š chunks ç”Ÿæˆç­”æ¡ˆ"""
    start = time.time()
    try:
        response = await generate_answer(question, chunks)
        latency = int((time.time() - start) * 1000)
        return GenerationResult(
            method=method_name,
            answer=response.answer,
            confidence=response.confidence,
            latency_ms=latency,
            citation_count=len(response.citations),
        )
    except Exception as e:
        return GenerationResult(
            method=method_name,
            answer=f"[ERROR: {e}]",
            confidence=0.0,
            latency_ms=int((time.time() - start) * 1000),
            citation_count=0,
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM è¯„åˆ¤å™¨: è‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆè´¨é‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def judge_answer_quality(
    question: str,
    answer_a: str,
    answer_b: str,
    label_a: str = "ç®€å•RAG",
    label_b: str = "å¢å¼ºRAG",
) -> dict:
    """
    è®© LLM å½“è£åˆ¤ï¼Œå¯¹æ¯”ä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡ã€‚
    è¿”å›: { winner, reason, score_a, score_b }
    """
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”ç³»ç»Ÿè¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹æ¯”ä»¥ä¸‹ä¸¤ä¸ªç³»ç»Ÿå¯¹åŒä¸€é—®é¢˜çš„å›ç­”è´¨é‡ã€‚

## ç”¨æˆ·é—®é¢˜
{question}

## ç­”æ¡ˆ A ({label_a})
{answer_a[:2000]}

## ç­”æ¡ˆ B ({label_b})
{answer_b[:2000]}

## è¯„ä¼°ç»´åº¦
1. **å‡†ç¡®æ€§**: ç­”æ¡ˆæ˜¯å¦å‡†ç¡®ã€æœ‰äº‹å®ä¾æ®
2. **å®Œæ•´æ€§**: æ˜¯å¦å›ç­”äº†é—®é¢˜çš„æ‰€æœ‰æ–¹é¢
3. **å¼•ç”¨è´¨é‡**: æ˜¯å¦æä¾›äº†æœ‰æ•ˆçš„æ¥æºå¼•ç”¨
4. **æ¸…æ™°åº¦**: ç­”æ¡ˆæ˜¯å¦æ¡ç†æ¸…æ™°ã€æ˜“äºç†è§£
5. **ç›¸å…³æ€§**: ç­”æ¡ˆæ˜¯å¦åˆ‡é¢˜ï¼Œæ²¡æœ‰å†—ä½™ä¿¡æ¯

## è¦æ±‚
è¿”å› JSON:
{{
  "score_a": 1-10,
  "score_b": 1-10,
  "winner": "A" æˆ– "B" æˆ– "tie",
  "reason": "ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆä¸€ä¸ªæ›´å¥½"
}}
"""
    try:
        result = await llm.generate_json(prompt)
        return result
    except Exception as e:
        return {"score_a": 5, "score_b": 5, "winner": "tie", "reason": f"è¯„ä¼°å¤±è´¥: {e}"}


async def judge_retrieval_relevance(
    question: str, chunks: list[RetrievedChunk]
) -> float:
    """è¯„ä¼°æ£€ç´¢ç»“æœå¯¹é—®é¢˜çš„æ•´ä½“ç›¸å…³æ€§ (0-1)"""
    if not chunks:
        return 0.0

    chunks_text = "\n---\n".join(
        f"[æ–‡æ¡£: {c.doc_title}, ç« èŠ‚: {c.section_path}]\n{c.content[:300]}"
        for c in chunks[:5]
    )
    prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹æ£€ç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„æ€»ä½“ç›¸å…³æ€§ã€‚

é—®é¢˜: {question}

æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ:
{chunks_text}

è¯·åªè¿”å›ä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„å°æ•°ï¼Œè¡¨ç¤ºç›¸å…³æ€§:
- 0.0-0.3: å‡ ä¹ä¸ç›¸å…³
- 0.3-0.5: éƒ¨åˆ†ç›¸å…³ï¼Œä½†ç¼ºå°‘å…³é”®ä¿¡æ¯
- 0.5-0.7: å¤§éƒ¨åˆ†ç›¸å…³
- 0.7-0.9: é«˜åº¦ç›¸å…³
- 0.9-1.0: å®Œç¾åŒ¹é…

åªè¿”å›æ•°å­—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
    try:
        text = await llm_light.generate(prompt, temperature=0.0)
        return float(text.strip())
    except Exception:
        return 0.5


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é»˜è®¤æµ‹è¯•é›†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def get_default_test_cases() -> list[TestCase]:
    """
    é€šç”¨æµ‹è¯•é—®é¢˜é›†ã€‚
    å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·æ›¿æ¢ä¸ºä½ è‡ªå·±æ–‡æ¡£åº“ä¸­çš„çœŸå®é—®é¢˜ã€‚
    """
    return [
        # â”€â”€ äº‹å®æ€§é—®é¢˜ (é€‚åˆæ¯”è¾ƒæ£€ç´¢ç²¾åº¦) â”€â”€
        TestCase(
            question="è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            category="factual",
        ),
        TestCase(
            question="æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›å…³é”®æ—¥æœŸæˆ–æ—¶é—´èŠ‚ç‚¹ï¼Ÿ",
            category="factual",
        ),
        TestCase(
            question="æ–‡æ¡£ä¸­çš„æ ¸å¿ƒç»“è®ºæˆ–å»ºè®®æ˜¯ä»€ä¹ˆï¼Ÿ",
            category="factual",
        ),

        # â”€â”€ æ€»ç»“æ€§é—®é¢˜ (é€‚åˆæ¯”è¾ƒä¸Šä¸‹æ–‡ç»„è£…) â”€â”€
        TestCase(
            question="è¯·æ€»ç»“è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œå…³é”®è¦ç‚¹",
            category="summary",
        ),
        TestCase(
            question="æ–‡æ¡£ä¸­å„ç« èŠ‚åˆ†åˆ«è®¨è®ºäº†å“ªäº›ä¸»é¢˜ï¼Ÿ",
            category="summary",
        ),

        # â”€â”€ åˆ†ææ€§é—®é¢˜ (é€‚åˆæ¯”è¾ƒæ·±åº¦ç†è§£) â”€â”€
        TestCase(
            question="æ–‡æ¡£ä¸­çš„æ•°æ®æˆ–æŒ‡æ ‡è¯´æ˜äº†ä»€ä¹ˆè¶‹åŠ¿ï¼Ÿ",
            category="comparison",
        ),
    ]


async def generate_test_cases_from_db() -> list[TestCase]:
    """ä»æ•°æ®åº“ä¸­è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼ˆåŸºäºå·²æœ‰æ–‡æ¡£ï¼‰"""
    test_cases = []
    async with get_db_session() as session:
        from sqlalchemy import text
        result = await session.execute(text("""
            SELECT doc_id, title, doc_summary
            FROM documents
            WHERE processing_status = 'ready' AND doc_summary IS NOT NULL
            ORDER BY created_at DESC
            LIMIT 5
        """))
        docs = result.fetchall()

    if not docs:
        print("âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å·²å¤„ç†çš„æ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•é›†")
        return get_default_test_cases()

    for doc_id, title, summary in docs:
        # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆé’ˆå¯¹æ€§é—®é¢˜
        test_cases.extend([
            TestCase(
                question=f"ã€Š{title}ã€‹è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                expected_doc_id=str(doc_id),
                category="factual",
            ),
            TestCase(
                question=f"è¯·æ€»ç»“ã€Š{title}ã€‹çš„æ ¸å¿ƒè¦ç‚¹",
                expected_doc_id=str(doc_id),
                category="summary",
            ),
        ])

    # å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£ï¼Œæ·»åŠ è·¨æ–‡æ¡£é—®é¢˜
    if len(docs) >= 2:
        titles = [row[1] for row in docs[:3]]
        test_cases.append(TestCase(
            question=f"å¯¹æ¯”ã€Š{titles[0]}ã€‹å’Œã€Š{titles[1]}ã€‹çš„ä¸»è¦å¼‚åŒç‚¹",
            category="comparison",
        ))

    return test_cases


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»è¯„ä¼°æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


RETRIEVAL_METHODS = {
    "A_ä»…å‘é‡": retrieval_vector_only,
    "B_ä»…BM25": retrieval_bm25_only,
    "C_æ··åˆæ— Rerank": retrieval_hybrid_no_rerank,
    "D_æ··åˆ+Rerank": retrieval_hybrid_with_rerank,
    "E_å®Œæ•´Pipeline": retrieval_full_pipeline,
}


async def run_retrieval_comparison(test_cases: list[TestCase]) -> list[EvalReport]:
    """è¿è¡Œæ£€ç´¢å¯¹æ¯”å®éªŒ"""
    reports = []

    for i, tc in enumerate(test_cases):
        print(f"\n{'='*60}")
        print(f"é—®é¢˜ {i+1}/{len(test_cases)}: {tc.question}")
        print(f"ç±»å‹: {tc.category}")
        print(f"{'='*60}")

        report = EvalReport(question=tc.question, category=tc.category)

        for method_name, method_fn in RETRIEVAL_METHODS.items():
            start = time.time()
            try:
                chunks = await method_fn(tc.question)
                latency = int((time.time() - start) * 1000)

                result = RetrievalResult(
                    method=method_name,
                    chunks=chunks,
                    latency_ms=latency,
                    top_scores=[c.score for c in chunks[:5]],
                )
                report.retrieval_results[method_name] = result

                # ç”¨ LLM è¯„ä¼°æ£€ç´¢ç›¸å…³æ€§
                relevance = await judge_retrieval_relevance(tc.question, chunks)
                report.relevance_scores[method_name] = relevance

                print(f"  {method_name}: {result.hit_count} chunks, "
                      f"max_score={result.max_score:.4f}, "
                      f"relevance={relevance:.2f}, "
                      f"{latency}ms")
            except Exception as e:
                print(f"  {method_name}: âŒ ERROR - {e}")

        reports.append(report)

    return reports


async def run_full_evaluation(test_cases: list[TestCase]) -> list[EvalReport]:
    """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆæ£€ç´¢ + ç”Ÿæˆ + LLM è¯„åˆ¤ï¼‰"""
    reports = await run_retrieval_comparison(test_cases)

    print(f"\n\n{'='*60}")
    print("å¼€å§‹ç­”æ¡ˆè´¨é‡å¯¹æ¯” (ç®€å•RAG vs å¢å¼ºRAG)")
    print(f"{'='*60}")

    for i, report in enumerate(reports):
        tc = test_cases[i]
        print(f"\né—®é¢˜: {tc.question}")

        # ç”¨æœ€ç®€å•æ£€ç´¢çš„ç»“æœç”Ÿæˆç­”æ¡ˆ (åŸºçº¿)
        baseline_chunks = report.retrieval_results.get("A_ä»…å‘é‡")
        full_chunks = report.retrieval_results.get("E_å®Œæ•´Pipeline")

        if baseline_chunks and full_chunks:
            baseline_gen = await generate_with_chunks(
                tc.question, baseline_chunks.chunks, "ç®€å•RAG(ä»…å‘é‡)"
            )
            full_gen = await generate_with_chunks(
                tc.question, full_chunks.chunks, "å¢å¼ºRAG(å®Œæ•´Pipeline)"
            )

            report.generation_results["baseline"] = baseline_gen
            report.generation_results["enhanced"] = full_gen

            # LLM å¯¹æ¯”è¯„åˆ¤
            if baseline_gen.answer and full_gen.answer and "[ERROR" not in baseline_gen.answer:
                judgment = await judge_answer_quality(
                    tc.question, baseline_gen.answer, full_gen.answer
                )
                print(f"  åŸºçº¿ç­”æ¡ˆ:  ç½®ä¿¡åº¦={baseline_gen.confidence:.2f}, {baseline_gen.latency_ms}ms")
                print(f"  å¢å¼ºç­”æ¡ˆ:  ç½®ä¿¡åº¦={full_gen.confidence:.2f}, {full_gen.latency_ms}ms")
                print(f"  LLM è¯„åˆ¤: {judgment.get('winner', '?')} è·èƒœ")
                print(f"    åŸºçº¿: {judgment.get('score_a', '?')}/10, "
                      f"å¢å¼º: {judgment.get('score_b', '?')}/10")
                print(f"    åŸå› : {judgment.get('reason', '?')}")
            else:
                print(f"  âš ï¸ è·³è¿‡ç­”æ¡ˆå¯¹æ¯”ï¼ˆç”Ÿæˆå‡ºé”™ï¼‰")

    return reports


def print_summary(reports: list[EvalReport]):
    """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
    print(f"\n\n{'='*60}")
    print("ğŸ“Š è¯„ä¼°æ±‡æ€»æŠ¥å‘Š")
    print(f"{'='*60}")

    # 1. æ£€ç´¢è´¨é‡æ±‡æ€»
    print(f"\n## æ£€ç´¢è´¨é‡å¯¹æ¯” (LLM è¯„ä¼°çš„ç›¸å…³æ€§ 0-1)")
    print(f"{'æ–¹æ³•':<20} {'å¹³å‡ç›¸å…³æ€§':>10} {'å¹³å‡å»¶è¿Ÿ':>10}")
    print("-" * 45)

    for method_name in RETRIEVAL_METHODS:
        scores = [r.relevance_scores.get(method_name, 0) for r in reports if method_name in r.relevance_scores]
        latencies = [r.retrieval_results[method_name].latency_ms for r in reports if method_name in r.retrieval_results]
        if scores:
            avg_score = sum(scores) / len(scores)
            avg_latency = sum(latencies) / len(latencies) if latencies else 0
            print(f"{method_name:<20} {avg_score:>10.3f} {avg_latency:>8.0f}ms")

    # 2. ç­”æ¡ˆè´¨é‡æ±‡æ€»
    baseline_confs = []
    enhanced_confs = []
    for r in reports:
        if "baseline" in r.generation_results:
            baseline_confs.append(r.generation_results["baseline"].confidence)
        if "enhanced" in r.generation_results:
            enhanced_confs.append(r.generation_results["enhanced"].confidence)

    if baseline_confs and enhanced_confs:
        print(f"\n## ç­”æ¡ˆè´¨é‡å¯¹æ¯”")
        print(f"  ç®€å• RAG å¹³å‡ç½®ä¿¡åº¦: {sum(baseline_confs)/len(baseline_confs):.3f}")
        print(f"  å¢å¼º RAG å¹³å‡ç½®ä¿¡åº¦: {sum(enhanced_confs)/len(enhanced_confs):.3f}")

    # 3. å…³é”®ç»“è®º
    print(f"\n## å¢å¼ºç‰¹æ€§è´¡çŒ®åˆ†æ")
    methods = list(RETRIEVAL_METHODS.keys())
    for i in range(1, len(methods)):
        prev = methods[i - 1]
        curr = methods[i]
        prev_scores = [r.relevance_scores.get(prev, 0) for r in reports if prev in r.relevance_scores]
        curr_scores = [r.relevance_scores.get(curr, 0) for r in reports if curr in r.relevance_scores]
        if prev_scores and curr_scores:
            delta = (sum(curr_scores) / len(curr_scores)) - (sum(prev_scores) / len(prev_scores))
            direction = "ğŸ“ˆ" if delta > 0.01 else ("ğŸ“‰" if delta < -0.01 else "â¡ï¸")
            feature = {
                "B_ä»…BM25": "+ BM25å…³é”®è¯æ£€ç´¢",
                "C_æ··åˆæ— Rerank": "+ RRFèåˆ",
                "D_æ··åˆ+Rerank": "+ Rerankeré‡æ’",
                "E_å®Œæ•´Pipeline": "+ ä¸Šä¸‹æ–‡æ‰©å±•",
            }.get(curr, curr)
            print(f"  {direction} {feature}: ç›¸å…³æ€§å˜åŒ– {delta:+.3f}")

    print(f"\n{'='*60}")
    print("ğŸ’¡ æç¤º: ç”¨æ›´å¤šé’ˆå¯¹æ€§çš„æµ‹è¯•é—®é¢˜å¯ä»¥å¾—åˆ°æ›´å‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚")
    print("   å»ºè®®: åˆ›å»º tests/eval_questions.jsonï¼ŒåŒ…å« 20+ ä¸ªæ ‡æ³¨äº†æ ‡å‡†ç­”æ¡ˆçš„é—®é¢˜ã€‚")
    print(f"{'='*60}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI å…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def main():
    parser = argparse.ArgumentParser(description="DocAI RAG è´¨é‡è¯„ä¼°")
    parser.add_argument("--retrieval-only", action="store_true", help="åªè¿è¡Œæ£€ç´¢å¯¹æ¯”ï¼Œä¸ç”Ÿæˆç­”æ¡ˆ")
    parser.add_argument("--test-file", type=str, help="è‡ªå®šä¹‰æµ‹è¯•é›†æ–‡ä»¶ (JSON)")
    parser.add_argument("--auto", action="store_true", help="è‡ªåŠ¨ä»æ•°æ®åº“ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
    args = parser.parse_args()

    print("ğŸ§ª DocAI RAG è´¨é‡è¯„ä¼°å·¥å…·")
    print("=" * 60)

    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    if args.test_file:
        with open(args.test_file) as f:
            data = json.load(f)
        test_cases = [TestCase(**tc) for tc in data]
        print(f"ä» {args.test_file} åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    elif args.auto:
        test_cases = await generate_test_cases_from_db()
        print(f"è‡ªåŠ¨ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    else:
        test_cases = await generate_test_cases_from_db()
        print(f"ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")

    # è¿è¡Œè¯„ä¼°
    if args.retrieval_only:
        reports = await run_retrieval_comparison(test_cases)
    else:
        reports = await run_full_evaluation(test_cases)

    # æ‰“å°æ±‡æ€»
    print_summary(reports)


if __name__ == "__main__":
    asyncio.run(main())
