"""
DocAI Platform - Phase 1 ç«¯åˆ°ç«¯éªŒè¯è„šæœ¬
éªŒè¯ï¼šè§£æ â†’ åˆ†å— â†’ åµŒå…¥ â†’ å­˜å‚¨ â†’ æ£€ç´¢ â†’ ç”Ÿæˆ å…¨æµç¨‹
éœ€è¦åŸºç¡€è®¾æ–½æœåŠ¡è¿è¡Œï¼ˆmake up && make initï¼‰
"""

from __future__ import annotations

import asyncio
import os
import sys
import time
from pathlib import Path

import structlog

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.insert(0, str(Path(__file__).parent.parent))

logger = structlog.get_logger()

TEST_DOCS_DIR = Path(__file__).parent.parent / "tests" / "test_docs"


class Phase1Validator:
    """Phase 1 ç«¯åˆ°ç«¯éªŒè¯"""

    def __init__(self):
        self.results: list[dict] = []
        self.doc_id: str | None = None

    def _record(self, step: str, passed: bool, detail: str = "", duration_ms: int = 0):
        status = "PASS" if passed else "FAIL"
        self.results.append({
            "step": step,
            "status": status,
            "detail": detail,
            "duration_ms": duration_ms,
        })
        icon = "âœ…" if passed else "âŒ"
        print(f"  {icon} {step}: {detail} ({duration_ms}ms)")

    async def run_all(self):
        """è¿è¡Œæ‰€æœ‰éªŒè¯æ­¥éª¤"""
        print("=" * 70)
        print("  DocAI Platform - Phase 1 ç«¯åˆ°ç«¯éªŒè¯")
        print("=" * 70)
        print()

        # Step 1: æ£€æŸ¥åŸºç¡€è®¾æ–½
        print("[1/7] åŸºç¡€è®¾æ–½è¿æ¥æ£€æŸ¥")
        await self._check_infrastructure()

        # Step 2: æ–‡æ¡£è§£æéªŒè¯
        print("\n[2/7] æ–‡æ¡£è§£æéªŒè¯")
        self._check_parser()

        # Step 3: åˆ†å—éªŒè¯
        print("\n[3/7] è¯­ä¹‰åˆ†å—éªŒè¯")
        self._check_chunker()

        # Step 4: åµŒå…¥è®¡ç®—éªŒè¯
        print("\n[4/7] åµŒå…¥æ¨¡å‹éªŒè¯")
        self._check_embedding()

        # Step 5: ç«¯åˆ°ç«¯å…¥åº“éªŒè¯ï¼ˆéœ€è¦åŸºç¡€è®¾æ–½ï¼‰
        print("\n[5/7] æ–‡æ¡£å…¥åº“ Pipeline éªŒè¯")
        await self._check_ingestion_pipeline()

        # Step 6: æ£€ç´¢éªŒè¯
        print("\n[6/7] æ··åˆæ£€ç´¢éªŒè¯")
        await self._check_retrieval()

        # Step 7: ç­”æ¡ˆç”ŸæˆéªŒè¯
        print("\n[7/7] ç­”æ¡ˆç”ŸæˆéªŒè¯")
        await self._check_generation()

        # æ¸…ç†
        if self.doc_id:
            print("\n[æ¸…ç†] åˆ é™¤æµ‹è¯•æ–‡æ¡£...")
            await self._cleanup()

        # æ±‡æ€»
        self._print_summary()

    async def _check_infrastructure(self):
        """æ£€æŸ¥æ‰€æœ‰åŸºç¡€è®¾æ–½æœåŠ¡æ˜¯å¦å¯ç”¨"""
        from app.core.infrastructure import (
            get_qdrant_client, get_es_client, get_redis_client, get_minio_client,
        )
        from sqlalchemy import text
        from app.core.infrastructure import get_db_session

        # PostgreSQL
        t0 = time.time()
        try:
            async with get_db_session() as session:
                result = await session.execute(text("SELECT 1"))
                result.scalar()
            self._record("PostgreSQL", True, "è¿æ¥æ­£å¸¸", int((time.time() - t0) * 1000))
        except Exception as e:
            self._record("PostgreSQL", False, str(e), int((time.time() - t0) * 1000))

        # Qdrant
        t0 = time.time()
        try:
            qdrant = get_qdrant_client()
            collections = await qdrant.get_collections()
            names = [c.name for c in collections.collections]
            self._record("Qdrant", True, f"é›†åˆ: {names}", int((time.time() - t0) * 1000))
        except Exception as e:
            self._record("Qdrant", False, str(e), int((time.time() - t0) * 1000))

        # Elasticsearch
        t0 = time.time()
        try:
            es = get_es_client()
            info = await es.info()
            version = info.get("version", {}).get("number", "?")
            self._record("Elasticsearch", True, f"ç‰ˆæœ¬: {version}", int((time.time() - t0) * 1000))
        except Exception as e:
            self._record("Elasticsearch", False, str(e), int((time.time() - t0) * 1000))

        # MinIO
        t0 = time.time()
        try:
            minio = get_minio_client()
            buckets = [b.name for b in minio.list_buckets()]
            self._record("MinIO", True, f"Buckets: {buckets}", int((time.time() - t0) * 1000))
        except Exception as e:
            self._record("MinIO", False, str(e), int((time.time() - t0) * 1000))

        # Redis
        t0 = time.time()
        try:
            redis = get_redis_client()
            await redis.ping()
            self._record("Redis", True, "è¿æ¥æ­£å¸¸", int((time.time() - t0) * 1000))
        except Exception as e:
            self._record("Redis", False, str(e), int((time.time() - t0) * 1000))

    def _check_parser(self):
        """éªŒè¯æ–‡æ¡£è§£æå™¨"""
        from app.ingestion.parser import parse_document

        supported = {".pdf", ".docx", ".pptx", ".xlsx"}
        test_files = [f for f in TEST_DOCS_DIR.iterdir() if f.suffix.lower() in supported]

        if not test_files:
            self._record("è§£æå™¨", False, "æœªæ‰¾åˆ°æµ‹è¯•æ–‡æ¡£")
            return

        success = 0
        for f in test_files:
            t0 = time.time()
            try:
                doc = parse_document(str(f))
                duration = int((time.time() - t0) * 1000)
                self._record(
                    f"è§£æ {f.name}",
                    True,
                    f"sections={len(doc.sections)}, tables={len(doc.tables)}, pages={doc.page_count}",
                    duration,
                )
                success += 1
            except Exception as e:
                duration = int((time.time() - t0) * 1000)
                self._record(f"è§£æ {f.name}", False, str(e)[:100], duration)

        rate = success / len(test_files) * 100
        self._record("è§£ææˆåŠŸç‡", rate >= 80, f"{success}/{len(test_files)} ({rate:.0f}%)")

    def _check_chunker(self):
        """éªŒè¯è¯­ä¹‰åˆ†å—"""
        from app.ingestion.parser import parse_document
        from app.ingestion.chunker import semantic_chunk

        test_file = next(
            (f for f in TEST_DOCS_DIR.iterdir() if f.suffix == ".docx"),
            None,
        )
        if not test_file:
            self._record("åˆ†å—å™¨", False, "æœªæ‰¾åˆ° DOCX æµ‹è¯•æ–‡æ¡£")
            return

        t0 = time.time()
        doc = parse_document(str(test_file))
        chunks = semantic_chunk(doc, doc_id="validation-test")
        duration = int((time.time() - t0) * 1000)

        self._record(
            f"åˆ†å— {test_file.name}",
            len(chunks) > 0,
            f"chunks={len(chunks)}, total_tokens={sum(c.token_count for c in chunks)}",
            duration,
        )

        # éªŒè¯ chunk å¤§å°
        oversized = [c for c in chunks if c.token_count > 1600]
        self._record(
            "Chunk å¤§å°åˆè§„",
            len(oversized) == 0,
            f"è¶…å¤§chunk: {len(oversized)}/{len(chunks)}",
        )

    def _check_embedding(self):
        """éªŒè¯åµŒå…¥æ¨¡å‹"""
        from app.core.embedding import encode_texts, encode_single
        from config.settings import settings

        t0 = time.time()
        try:
            vec = encode_single("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯åµŒå…¥æ¨¡å‹ã€‚")
            duration = int((time.time() - t0) * 1000)
            self._record(
                "å•æ–‡æœ¬åµŒå…¥",
                len(vec) == settings.embedding_dimension,
                f"ç»´åº¦={len(vec)}",
                duration,
            )
        except Exception as e:
            self._record("å•æ–‡æœ¬åµŒå…¥", False, str(e), int((time.time() - t0) * 1000))

        t0 = time.time()
        try:
            vecs = encode_texts([f"æµ‹è¯•æ–‡æœ¬ {i}" for i in range(10)])
            duration = int((time.time() - t0) * 1000)
            self._record(
                "æ‰¹é‡åµŒå…¥ (10æ¡)",
                len(vecs) == 10,
                f"æ•°é‡={len(vecs)}, ç»´åº¦={len(vecs[0])}",
                duration,
            )
        except Exception as e:
            self._record("æ‰¹é‡åµŒå…¥", False, str(e), int((time.time() - t0) * 1000))

    async def _check_ingestion_pipeline(self):
        """éªŒè¯å®Œæ•´å…¥åº“æµç¨‹"""
        from app.ingestion.pipeline import ingestion_pipeline

        # é€‰ä¸€ä¸ªå°æ–‡ä»¶æµ‹è¯•
        test_file = next(
            (f for f in TEST_DOCS_DIR.iterdir() if f.suffix == ".docx"),
            None,
        )
        if not test_file:
            self._record("å…¥åº“ Pipeline", False, "æœªæ‰¾åˆ°æµ‹è¯•æ–‡æ¡£")
            return

        t0 = time.time()
        try:
            self.doc_id = await ingestion_pipeline.process_document(
                file_path=str(test_file),
                original_filename=test_file.name,
                doc_type="test",
                tags=["validation", "phase1"],
            )
            duration = int((time.time() - t0) * 1000)
            self._record("å…¥åº“ Pipeline", True, f"doc_id={self.doc_id}", duration)
        except Exception as e:
            duration = int((time.time() - t0) * 1000)
            self._record("å…¥åº“ Pipeline", False, str(e)[:200], duration)

    async def _check_retrieval(self):
        """éªŒè¯æ··åˆæ£€ç´¢"""
        if not self.doc_id:
            self._record("æ··åˆæ£€ç´¢", False, "æ— å¯æ£€ç´¢çš„æ–‡æ¡£ï¼ˆå…¥åº“å¤±è´¥ï¼‰")
            return

        from app.retrieval.hybrid_search import hybrid_search

        test_queries = [
            "è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æœ‰å“ªäº›å…³é”®è¦æ±‚ï¼Ÿ",
        ]

        for query in test_queries:
            t0 = time.time()
            try:
                results = await hybrid_search(
                    query=query,
                    doc_id=self.doc_id,
                    top_k=3,
                )
                duration = int((time.time() - t0) * 1000)
                self._record(
                    f"æ£€ç´¢: {query[:20]}...",
                    len(results) > 0,
                    f"ç»“æœæ•°={len(results)}, top_score={results[0].score:.3f}" if results else "æ— ç»“æœ",
                    duration,
                )
            except Exception as e:
                duration = int((time.time() - t0) * 1000)
                self._record(f"æ£€ç´¢: {query[:20]}...", False, str(e)[:100], duration)

    async def _check_generation(self):
        """éªŒè¯ç­”æ¡ˆç”Ÿæˆ"""
        if not self.doc_id:
            self._record("ç­”æ¡ˆç”Ÿæˆ", False, "æ— å¯æ£€ç´¢çš„æ–‡æ¡£")
            return

        from app.retrieval.hybrid_search import hybrid_search
        from app.generation.answer import generate_answer

        query = "è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"

        t0 = time.time()
        try:
            chunks = await hybrid_search(query=query, doc_id=self.doc_id, top_k=3)
            response = await generate_answer(question=query, retrieved_chunks=chunks)
            duration = int((time.time() - t0) * 1000)

            self._record(
                "ç­”æ¡ˆç”Ÿæˆ",
                len(response.answer) > 20,
                f"ç­”æ¡ˆé•¿åº¦={len(response.answer)}, å¼•ç”¨æ•°={len(response.citations)}, ç½®ä¿¡åº¦={response.confidence}",
                duration,
            )
        except Exception as e:
            duration = int((time.time() - t0) * 1000)
            self._record("ç­”æ¡ˆç”Ÿæˆ", False, str(e)[:200], duration)

    async def _cleanup(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        if not self.doc_id:
            return
        try:
            from app.ingestion.pipeline import ingestion_pipeline
            await ingestion_pipeline.delete_document(self.doc_id)
            print(f"  âœ… å·²æ¸…ç†æµ‹è¯•æ–‡æ¡£ {self.doc_id}")
        except Exception as e:
            print(f"  âš ï¸ æ¸…ç†å¤±è´¥: {e}")

    def _print_summary(self):
        """æ‰“å°éªŒè¯æ±‡æ€»"""
        print("\n" + "=" * 70)
        print("  éªŒè¯æ±‡æ€»")
        print("=" * 70)

        passed = sum(1 for r in self.results if r["status"] == "PASS")
        failed = sum(1 for r in self.results if r["status"] == "FAIL")
        total = len(self.results)

        print(f"\n  æ€»è®¡: {total} é¡¹æ£€æŸ¥")
        print(f"  é€šè¿‡: {passed} âœ…")
        print(f"  å¤±è´¥: {failed} âŒ")
        print(f"  é€šè¿‡ç‡: {passed / total * 100:.0f}%")

        if failed > 0:
            print("\n  å¤±è´¥é¡¹:")
            for r in self.results:
                if r["status"] == "FAIL":
                    print(f"    âŒ {r['step']}: {r['detail']}")

        print()

        # Phase 1 éªŒè¯æ ‡å‡†å¯¹ç…§
        print("  Phase 1 éªŒè¯æ ‡å‡†å¯¹ç…§:")
        print("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("  â”‚ æŒ‡æ ‡                   â”‚ ç›®æ ‡   â”‚ çŠ¶æ€                     â”‚")
        print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

        parse_results = [r for r in self.results if "è§£æ" in r["step"]]
        parse_ok = all(r["status"] == "PASS" for r in parse_results) if parse_results else False
        print(f"  â”‚ æ–‡æ¡£è§£ææˆåŠŸç‡         â”‚ â‰¥ 95%  â”‚ {'âœ… é€šè¿‡' if parse_ok else 'âŒ æœªè¾¾æ ‡':25s}â”‚")

        retrieval_results = [r for r in self.results if "æ£€ç´¢" in r["step"]]
        retrieval_ok = any(r["status"] == "PASS" for r in retrieval_results) if retrieval_results else False
        print(f"  â”‚ æ£€ç´¢å¬å›ç‡ Recall@5    â”‚ â‰¥ 80%  â”‚ {'âœ… åŠŸèƒ½å°±ç»ª' if retrieval_ok else 'â³ éœ€åœ¨çº¿éªŒè¯':25s}â”‚")

        gen_results = [r for r in self.results if "ç­”æ¡ˆ" in r["step"]]
        gen_ok = any(r["status"] == "PASS" for r in gen_results) if gen_results else False
        print(f"  â”‚ ç­”æ¡ˆå‡†ç¡®ç‡             â”‚ â‰¥ 75%  â”‚ {'âœ… åŠŸèƒ½å°±ç»ª' if gen_ok else 'â³ éœ€åœ¨çº¿éªŒè¯':25s}â”‚")

        pipeline_results = [r for r in self.results if "Pipeline" in r["step"]]
        pipeline_duration = next((r["duration_ms"] for r in pipeline_results if r["status"] == "PASS"), 0)
        print(f"  â”‚ å•æ–‡æ¡£å¤„ç†æ—¶é—´         â”‚ < 60s  â”‚ {'âœ… ' + str(pipeline_duration) + 'ms' if pipeline_duration > 0 else 'â³ éœ€åœ¨çº¿éªŒè¯':25s}â”‚")

        query_durations = [r["duration_ms"] for r in retrieval_results + gen_results if r["status"] == "PASS"]
        avg_query = sum(query_durations) // len(query_durations) if query_durations else 0
        print(f"  â”‚ æŸ¥è¯¢å“åº”æ—¶é—´           â”‚ < 5s   â”‚ {'âœ… ~' + str(avg_query) + 'ms' if avg_query > 0 else 'â³ éœ€åœ¨çº¿éªŒè¯':25s}â”‚")

        print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        print()

        if failed == 0:
            print("  ğŸ‰ Phase 1 æ‰€æœ‰éªŒè¯é¡¹é€šè¿‡ï¼")
        elif failed <= 2:
            print("  âš ï¸ Phase 1 åŸºæœ¬å®Œæˆï¼Œéƒ¨åˆ†éªŒè¯é¡¹éœ€åœ¨çº¿ç¯å¢ƒç¡®è®¤ã€‚")
        else:
            print("  âŒ Phase 1 å­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œè¯·æ£€æŸ¥åŸºç¡€è®¾æ–½å’Œä»£ç ã€‚")


async def main():
    validator = Phase1Validator()
    await validator.run_all()


if __name__ == "__main__":
    asyncio.run(main())
