# AI ä¼ä¸šçº§æ–‡æ¡£ç®¡ç†ç³»ç»Ÿâ€”â€”åˆ†é˜¶æ®µå®æ–½æ–¹æ¡ˆ

---

## æ€»ä½“è§„åˆ’è·¯çº¿å›¾

```
Phase 0 â”€â”€â”€ åŸºç¡€è®¾æ–½ & æŠ€æœ¯é€‰å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [ç¬¬ 1-2 å‘¨]
Phase 1 â”€â”€â”€ å•æ–‡æ¡£æ£€ç´¢é—®ç­”ï¼ˆMVPï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [ç¬¬ 3-6 å‘¨]
Phase 2 â”€â”€â”€ å¤šæ–‡æ¡£æ£€ç´¢ & è·¨æ–‡æ¡£æ€»ç»“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [ç¬¬ 7-12 å‘¨]
Phase 3 â”€â”€â”€ ç‰ˆæœ¬ç®¡ç† & å·®å¼‚å¯¹æ¯” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [ç¬¬ 13-18 å‘¨]
Phase 4 â”€â”€â”€ æ™ºèƒ½ç¼–æ’ & Agent åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [ç¬¬ 19-24 å‘¨]
Phase 5 â”€â”€â”€ ç”Ÿäº§åŒ–åŠ å›º & æŒç»­ä¼˜åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [ç¬¬ 25-30 å‘¨]
```

æ¯ä¸ª Phase éƒ½å¯ä»¥ç‹¬ç«‹äº¤ä»˜å¯ç”¨çš„ç³»ç»Ÿï¼Œåä¸€ä¸ª Phase åœ¨å‰ä¸€ä¸ªçš„åŸºç¡€ä¸Šå¢é‡å»ºè®¾ã€‚

---

## Phase 0ï¼šåŸºç¡€è®¾æ–½ & æŠ€æœ¯é€‰å‹ï¼ˆç¬¬ 1-2 å‘¨ï¼‰

### 0.1 ç›®æ ‡

ç¡®å®šæŠ€æœ¯æ ˆã€æ­å»ºå¼€å‘ç¯å¢ƒã€å‡†å¤‡æµ‹è¯•æ–‡æ¡£é›†ã€‚
æœ¬é˜¶æ®µä¸å†™ä¸šåŠ¡ä»£ç ï¼Œåªåšå†³ç­–å’ŒåŸºå»ºã€‚

### 0.2 æŠ€æœ¯é€‰å‹å†³ç­–æ¸…å•

| ç»„ä»¶              | æ¨èæ–¹æ¡ˆ                 | å¤‡é€‰æ–¹æ¡ˆ                     | å†³ç­–ä¾æ®                     |
| ----------------- | ------------------------ | ---------------------------- | ---------------------------- |
| **LLM**           | Claude Sonnet 4 (API)    | GPT-4o / å¼€æº Qwen2.5-72B    | ä¸­æ–‡èƒ½åŠ›ã€é•¿ä¸Šä¸‹æ–‡ã€æˆæœ¬å¹³è¡¡ |
| **åµŒå…¥æ¨¡å‹**      | BGE-M3 (è‡ªéƒ¨ç½²)          | Cohere Embed v3 / Jina v3    | ä¸­æ–‡å¤šè¯­è¨€æ”¯æŒã€å¯ç§æœ‰åŒ–éƒ¨ç½² |
| **å‘é‡æ•°æ®åº“**    | Qdrant (Docker è‡ªéƒ¨ç½²)   | Weaviate / pgvector          | æ€§èƒ½å¥½ã€æ··åˆæ£€ç´¢åŸç”Ÿæ”¯æŒ     |
| **å…¨æ–‡æœç´¢**      | Elasticsearch 8.x        | OpenSearch                   | ä¸­æ–‡åˆ†è¯ (IK)ã€æˆç†Ÿç¨³å®š      |
| **Reranker**      | BGE-Reranker-v2-m3       | Cohere Rerank v3             | ä¸­æ–‡æ•ˆæœå¥½ã€å¯ç§æœ‰åŒ–         |
| **æ–‡æ¡£è§£æ**      | Docling + PyMuPDF        | Unstructured.io / LlamaParse | å¼€æºã€è¡¨æ ¼è¯†åˆ«èƒ½åŠ›å¼º         |
| **OCRï¼ˆæ‰«æä»¶ï¼‰** | Surya / PaddleOCR        | Azure Document Intelligence  | ä¸­æ–‡è¯†åˆ«ç‡é«˜                 |
| **åº”ç”¨æ¡†æ¶**      | LlamaIndex + FastAPI     | LangChain + LangGraph        | LlamaIndex æ–‡æ¡£å¤„ç†æˆç†Ÿ      |
| **å…ƒæ•°æ®åº“**      | PostgreSQL 16            | MongoDB                      | ç‰ˆæœ¬å…³ç³»é€‚åˆå…³ç³»å‹           |
| **å¯¹è±¡å­˜å‚¨**      | MinIO (è‡ªéƒ¨ç½²)           | é˜¿é‡Œäº‘ OSS / AWS S3          | å­˜æ”¾æ–‡æ¡£åŸæ–‡                 |
| **å‰ç«¯**          | Next.js + React          | Vue 3                        | ç”Ÿæ€ä¸°å¯Œ                     |
| **æ¶ˆæ¯é˜Ÿåˆ—**      | Redis Streams / RabbitMQ | Kafka (å¦‚æ–‡æ¡£é‡æå¤§)         | å¼‚æ­¥ä»»åŠ¡åˆ†å‘                 |

### 0.3 ç¯å¢ƒæ­å»º

```bash
# docker-compose.yml æ ¸å¿ƒæœåŠ¡
services:
  qdrant:        # å‘é‡æ•°æ®åº“
  elasticsearch: # å…¨æ–‡æœç´¢ + IK ä¸­æ–‡åˆ†è¯
  postgres:      # å…ƒæ•°æ® + ç‰ˆæœ¬ç®¡ç†
  minio:         # æ–‡æ¡£åŸæ–‡å­˜å‚¨
  redis:         # ç¼“å­˜ + ä»»åŠ¡é˜Ÿåˆ—
  embedding:     # BGE-M3 æ¨¡å‹æœåŠ¡ (GPU)
  reranker:      # BGE-Reranker æœåŠ¡ (GPU)
  api:           # FastAPI åç«¯
  web:           # Next.js å‰ç«¯
```

### 0.4 æµ‹è¯•æ–‡æ¡£é›†å‡†å¤‡

- å‡†å¤‡ 3 ç±»æ–‡æ¡£å„ 10 ä»½ï¼ˆåˆåŒ/æŠ¥å‘Š/æ”¿ç­–ç­‰ï¼‰ï¼Œè¦†ç›–ï¼š
  - çŸ­æ–‡æ¡£ (< 10 é¡µ) å’Œé•¿æ–‡æ¡£ (50+ é¡µ)
  - å«è¡¨æ ¼ã€å›¾è¡¨çš„å¤æ‚ç‰ˆé¢
  - åŒä¸€æ–‡æ¡£çš„ 2-3 ä¸ªä¸åŒç‰ˆæœ¬
- ä¸ºæ¯ç±»æ–‡æ¡£å‡†å¤‡ 20 ä¸ªæµ‹è¯•é—®é¢˜ï¼ˆæ¶µç›–äº‹å®æŸ¥è¯¢ã€æ€»ç»“ã€å¯¹æ¯”ï¼‰
- è¿™ä¸ªæµ‹è¯•é›†å°†è´¯ç©¿æ‰€æœ‰ Phase çš„éªŒè¯

### 0.5 äº¤ä»˜ç‰©

- [x] æŠ€æœ¯é€‰å‹å†³ç­–æ–‡æ¡£
- [x] docker-compose å¼€å‘ç¯å¢ƒä¸€é”®å¯åŠ¨
- [x] æµ‹è¯•æ–‡æ¡£é›† + æ ‡æ³¨å¥½çš„æµ‹è¯•é—®é¢˜é›†
- [x] é¡¹ç›®ä»£ç ä»“åº“åˆå§‹åŒ–

---

## Phase 1ï¼šå•æ–‡æ¡£æ£€ç´¢é—®ç­”â€”â€”MVPï¼ˆç¬¬ 3-6 å‘¨ï¼‰

### 1.1 ç›®æ ‡

å®ç°æ ¸å¿ƒ RAG pipelineï¼šç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ â†’ è§£æ â†’ åˆ†å— â†’ åµŒå…¥ â†’ æ£€ç´¢ â†’ é—®ç­”ã€‚
è¿™æ˜¯æœ€å°å¯ç”¨äº§å“ï¼Œè§£å†³"ä»æ–‡æ¡£ä¸­è¯»å–éƒ¨åˆ†ç›¸å…³ä¿¡æ¯"çš„åŸºæœ¬éœ€æ±‚ã€‚

### 1.2 åŠŸèƒ½èŒƒå›´

```
ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£(PDF/Word/PPT)
    â”‚
    â–¼
[æ–‡æ¡£è§£ææ¨¡å—] â”€â”€â†’ æå–æ–‡æœ¬ + ä¿ç•™ç»“æ„ï¼ˆæ ‡é¢˜å±‚çº§ã€é¡µç ã€è¡¨æ ¼ï¼‰
    â”‚
    â–¼
[åˆ†å—æ¨¡å—] â”€â”€â†’ è¯­ä¹‰åˆ†å—ï¼ˆæŒ‰æ ‡é¢˜/æ®µè½è‡ªç„¶åˆ‡åˆ†ï¼Œ300-800 tokensï¼‰
    â”‚                â””â”€ æ¯ä¸ª chunk æºå¸¦ metadata:
    â”‚                     - doc_id, doc_title
    â”‚                     - section_path (å¦‚: "ç¬¬ä¸‰ç«  > 3.2 ä»˜æ¬¾æ¡æ¬¾")
    â”‚                     - page_number
    â”‚                     - chunk_index (åœ¨æ–‡æ¡£ä¸­çš„é¡ºåº)
    â–¼
[åµŒå…¥æ¨¡å—] â”€â”€â†’ BGE-M3 ç”Ÿæˆå‘é‡
    â”‚
    â–¼
[å­˜å‚¨æ¨¡å—] â”€â”€â†’ å‘é‡ â†’ Qdrant
             â”€â”€â†’ å…¨æ–‡ â†’ Elasticsearch
             â”€â”€â†’ åŸæ–‡ â†’ MinIO
             â”€â”€â†’ å…ƒæ•°æ® â†’ PostgreSQL
    â”‚
    â–¼
[æ£€ç´¢æ¨¡å—] â”€â”€â†’ ç”¨æˆ·æé—®
             â”€â”€â†’ å‘é‡æ£€ç´¢ (top 20) + BM25 æ£€ç´¢ (top 20)
             â”€â”€â†’ RRF èåˆ â†’ Reranker é‡æ’ â†’ top 5
    â”‚
    â–¼
[ç”Ÿæˆæ¨¡å—] â”€â”€â†’ Prompt = System Prompt + æ£€ç´¢åˆ°çš„ chunks + ç”¨æˆ·é—®é¢˜
             â”€â”€â†’ LLM ç”Ÿæˆç­”æ¡ˆï¼ˆè¦æ±‚é™„å¸¦å¼•ç”¨æ¥æºï¼‰
             â”€â”€â†’ è¿”å›ç­”æ¡ˆ + å¼•ç”¨é“¾æ¥ï¼ˆå¯è·³è½¬åˆ°åŸæ–‡å¯¹åº”ä½ç½®ï¼‰
```

### 1.3 å…³é”®å®ç°ç»†èŠ‚

**1.3.1 æ–‡æ¡£è§£æâ€”â€”éœ€è¦å¤„ç†å¥½çš„è¾¹ç•Œæƒ…å†µï¼š**

```python
# è§£æç­–ç•¥è·¯ç”±
def parse_document(file_path: str) -> ParsedDocument:
    ext = get_extension(file_path)
    
    if ext == '.pdf':
        if is_scanned_pdf(file_path):
            return parse_with_ocr(file_path)      # Surya/PaddleOCR
        else:
            return parse_with_pymupdf(file_path)   # PyMuPDF æå–æ–‡å­—
    elif ext in ['.docx', '.doc']:
        return parse_with_docling(file_path)
    elif ext in ['.pptx']:
        return parse_with_docling(file_path)
    elif ext in ['.xlsx', '.csv']:
        return parse_spreadsheet(file_path)        # è¡¨æ ¼â†’Markdown
    else:
        raise UnsupportedFormat(ext)
```

**1.3.2 è¯­ä¹‰åˆ†å—â€”â€”æ ¸å¿ƒé€»è¾‘ï¼š**

```python
def semantic_chunk(parsed_doc: ParsedDocument) -> list[Chunk]:
    chunks = []
    for section in parsed_doc.sections:
        # æŒ‰æ ‡é¢˜å±‚çº§è‡ªç„¶åˆ‡åˆ†
        if section.token_count <= 800:
            # å°äºé˜ˆå€¼ï¼Œæ•´æ®µä½œä¸ºä¸€ä¸ª chunk
            chunks.append(make_chunk(section, parsed_doc))
        else:
            # å¤§äºé˜ˆå€¼ï¼ŒæŒ‰æ®µè½è¾¹ç•Œè¿›ä¸€æ­¥åˆ‡åˆ†
            sub_chunks = split_by_paragraphs(section, 
                                              target_size=500, 
                                              overlap=50)
            chunks.extend(sub_chunks)
    
    # è¡¨æ ¼å•ç‹¬æˆ chunk
    for table in parsed_doc.tables:
        chunks.append(make_table_chunk(table, parsed_doc))
    
    return chunks
```

**1.3.3 Chunk Metadata ç»“æ„ï¼š**

```json
{
  "chunk_id": "uuid",
  "doc_id": "uuid",
  "doc_title": "XXä¾›åº”å•†åˆåŒ",
  "section_path": "ç¬¬å››ç«  ä»˜æ¬¾æ¡æ¬¾ > 4.2 ä»˜æ¬¾å‘¨æœŸ",
  "page_numbers": [12, 13],
  "chunk_index": 15,
  "chunk_type": "text",       // text | table | image_description
  "token_count": 520,
  "content": "å®é™…æ–‡æœ¬å†…å®¹...",
  "created_at": "2025-01-15T10:00:00Z"
}
```

### 1.4 å‰ç«¯ MVP

æ­¤é˜¶æ®µå‰ç«¯åªéœ€è¦ï¼š
- æ–‡æ¡£ä¸Šä¼ ç•Œé¢ï¼ˆæ‹–æ‹½ä¸Šä¼ ï¼Œæ˜¾ç¤ºå¤„ç†è¿›åº¦ï¼‰
- æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²ä¸Šä¼ çš„æ–‡æ¡£ï¼Œå¤„ç†çŠ¶æ€ï¼‰
- é—®ç­”ç•Œé¢ï¼ˆè¾“å…¥é—®é¢˜ â†’ å±•ç¤ºç­”æ¡ˆ + å¼•ç”¨æ¥æº + å¯ç‚¹å‡»è·³è½¬åŸæ–‡ï¼‰

### 1.5 éªŒè¯æ ‡å‡†

| æŒ‡æ ‡                | ç›®æ ‡          | æµ‹è¯•æ–¹æ³•                           |
| ------------------- | ------------- | ---------------------------------- |
| æ–‡æ¡£è§£ææˆåŠŸç‡      | â‰¥ 95%         | 30 ä»½æµ‹è¯•æ–‡æ¡£å…¨éƒ¨æ­£ç¡®è§£æ          |
| æ£€ç´¢å¬å›ç‡ Recall@5 | â‰¥ 80%         | æµ‹è¯•é—®é¢˜é›†ä¸­çš„æ­£ç¡®ç­”æ¡ˆå‡ºç°åœ¨ top 5 |
| ç­”æ¡ˆå‡†ç¡®ç‡          | â‰¥ 75%         | äººå·¥è¯„å®¡ç­”æ¡ˆçš„æ­£ç¡®æ€§               |
| å¼•ç”¨å‡†ç¡®ç‡          | â‰¥ 85%         | æ ‡æ³¨çš„æ¥æºç¡®å®åŒ…å«ç­”æ¡ˆä¾æ®         |
| å•æ–‡æ¡£å¤„ç†æ—¶é—´      | < 60 ç§’/10 é¡µ | ç«¯åˆ°ç«¯è®¡æ—¶                         |
| æŸ¥è¯¢å“åº”æ—¶é—´        | < 5 ç§’        | ä»æé—®åˆ°ç­”æ¡ˆè¿”å›                   |

### 1.6 äº¤ä»˜ç‰©

- [x] æ–‡æ¡£è§£æ Pipelineï¼ˆæ”¯æŒ PDF/Word/PPTï¼‰
- [x] åˆ†å— + åµŒå…¥ + åŒè·¯ç´¢å¼•ï¼ˆå‘é‡ + å…¨æ–‡ï¼‰
- [x] åŸºç¡€ RAG é—®ç­” API
- [x] ç®€æ˜“ Web ç•Œé¢
- [x] éªŒè¯æŠ¥å‘Šï¼ˆå«å„æŒ‡æ ‡å®æµ‹æ•°æ®ï¼‰

---

## Phase 2ï¼šå¤šæ–‡æ¡£æ£€ç´¢ & è·¨æ–‡æ¡£æ€»ç»“ï¼ˆç¬¬ 7-12 å‘¨ï¼‰

### 2.1 ç›®æ ‡

æ”¯æŒè·¨å¤šä¸ªæ–‡æ¡£çš„æ£€ç´¢å’Œæ€»ç»“æ€§é—®ç­”ã€‚
è§£å†³"å¯¹æŸä¸€ç±»ä¸šåŠ¡åšæ€»ç»“å¼æé—®"å’Œ"éœ€è¦ç»Ÿé¢†è§†è§’"çš„éœ€æ±‚ã€‚

### 2.2 æ–°å¢èƒ½åŠ›

```
Phase 1 å·²æœ‰               Phase 2 æ–°å¢
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å•æ–‡æ¡£ä¸Šä¼ è§£æ         â†’    æ‰¹é‡æ–‡æ¡£å¯¼å…¥ + æ–‡æ¡£åˆ†ç»„ç®¡ç†
åŸºç¡€ RAG æ£€ç´¢         â†’    æ··åˆæ£€ç´¢ + å…ƒæ•°æ®è¿‡æ»¤ + Query æ”¹å†™
å•è½®é—®ç­”             â†’    å¤šè½®å¯¹è¯ + ä¸Šä¸‹æ–‡è®°å¿†
åŸºç¡€ç­”æ¡ˆç”Ÿæˆ         â†’    è·¨æ–‡æ¡£æ€»ç»“ (Map-Reduce)
                          é¢„å»ºæ‘˜è¦å±‚ï¼ˆç« èŠ‚ + æ–‡æ¡£çº§ï¼‰
                          Contextual Retrievalï¼ˆä¸Šä¸‹æ–‡å¢å¼ºåµŒå…¥ï¼‰
```

### 2.3 å…³é”®æ¨¡å—å®ç°

**2.3.1 æ–‡æ¡£åˆ†ç»„ä¸å…ƒæ•°æ®å¢å¼º**

```sql
-- PostgreSQL è¡¨ç»“æ„
CREATE TABLE documents (
    doc_id          UUID PRIMARY KEY,
    title           TEXT NOT NULL,
    doc_type        VARCHAR(50),     -- åˆåŒ/æŠ¥å‘Š/æ”¿ç­–/...
    department      VARCHAR(100),    -- æ‰€å±éƒ¨é—¨
    tags            TEXT[],          -- æ ‡ç­¾æ•°ç»„
    group_id        UUID,            -- æ–‡æ¡£ç»„ï¼ˆå¦‚"2024å¹´åº¦å®¡è®¡"ï¼‰
    status          VARCHAR(20),     -- active/archived
    created_at      TIMESTAMPTZ,
    file_path       TEXT,            -- MinIO ä¸­çš„è·¯å¾„
    page_count      INT,
    -- Phase 2 æ–°å¢
    doc_summary     TEXT,            -- æ–‡æ¡£çº§æ‘˜è¦
    key_entities    JSONB,           -- æå–çš„å…³é”®å®ä½“
    CONSTRAINT fk_group FOREIGN KEY (group_id)
        REFERENCES document_groups(group_id)
);

CREATE TABLE document_groups (
    group_id    UUID PRIMARY KEY,
    name        TEXT NOT NULL,
    description TEXT,
    created_at  TIMESTAMPTZ
);

CREATE TABLE section_summaries (
    summary_id      UUID PRIMARY KEY,
    doc_id          UUID REFERENCES documents(doc_id),
    section_path    TEXT,
    summary_text    TEXT,        -- LLM ç”Ÿæˆçš„ç« èŠ‚æ‘˜è¦
    key_points      JSONB,      -- ç»“æ„åŒ–è¦ç‚¹
    token_count     INT,
    created_at      TIMESTAMPTZ
);
```

**2.3.2 é¢„å»ºæ‘˜è¦å±‚ï¼ˆç¦»çº¿å¤„ç†ï¼Œæ–‡æ¡£å…¥åº“æ—¶è§¦å‘ï¼‰**

```
æ–‡æ¡£å…¥åº“å®Œæˆï¼ˆPhase 1 çš„ chunk å·²å­˜å¥½ï¼‰
    â”‚
    â–¼
[ç« èŠ‚æ‘˜è¦ç”Ÿæˆ] å¯¹æ¯ä¸ªé¡¶çº§ç« èŠ‚ï¼š
    â”‚  Prompt: "è¯·å¯¹ä»¥ä¸‹ç« èŠ‚å†…å®¹ç”Ÿæˆ 200-300 å­—çš„æ‘˜è¦ï¼Œ
    â”‚           æå– 3-5 ä¸ªå…³é”®è¦ç‚¹ï¼Œè¯†åˆ«å…³é”®å®ä½“å’Œæ•°æ®ã€‚"
    â”‚  Input:  ç« èŠ‚ä¸‹æ‰€æœ‰ chunks æ‹¼æ¥
    â”‚  Output: { summary, key_points, entities }
    â”‚  â†’ å­˜å…¥ section_summaries è¡¨
    â”‚  â†’ summary æ–‡æœ¬ä¹ŸåšåµŒå…¥å­˜å…¥å‘é‡åº“ï¼ˆæ ‡è®° chunk_type=section_summaryï¼‰
    â”‚
    â–¼
[æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ] åŸºäºæ‰€æœ‰ç« èŠ‚æ‘˜è¦ï¼š
    â”‚  Prompt: "åŸºäºä»¥ä¸‹å„ç« èŠ‚æ‘˜è¦ï¼Œç”Ÿæˆæ•´ä»½æ–‡æ¡£çš„ 300-500 å­—æ€»ç»“ã€‚
    â”‚           åŒ…å«ï¼šæ–‡æ¡£ç±»å‹ã€æ ¸å¿ƒè¦ç‚¹(3-5ä¸ª)ã€å…³é”®æ•°å€¼/æ—¥æœŸã€æ¶‰åŠçš„ä¸»è¦å®ä½“ã€‚"
    â”‚  Input:  æ‰€æœ‰ section_summaries æ‹¼æ¥
    â”‚  Output: doc_summary
    â”‚  â†’ æ›´æ–° documents è¡¨çš„ doc_summary å­—æ®µ
    â”‚  â†’ doc_summary ä¹ŸåšåµŒå…¥å­˜å…¥å‘é‡åº“ï¼ˆæ ‡è®° chunk_type=doc_summaryï¼‰
    â”‚
    â–¼
[å®ä½“æå–] ä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼š
    Prompt: "ä»ä»¥ä¸‹æ–‡æ¡£æ‘˜è¦ä¸­æå–å…³é”®å®ä½“..."
    Output: { äººå:[], ç»„ç»‡:[], æ—¥æœŸ:[], é‡‘é¢:[], æ¡æ¬¾ç±»å‹:[] }
    â†’ å­˜å…¥ documents.key_entities
```

**2.3.3 Contextual Retrievalï¼ˆä¸Šä¸‹æ–‡å¢å¼ºåµŒå…¥ï¼‰**

åœ¨ Phase 1 çš„åŸºç¡€ä¸Šï¼Œä¸ºæ¯ä¸ª chunk å¢åŠ ä¸Šä¸‹æ–‡æè¿°ï¼š

```python
def add_contextual_description(chunk: Chunk, doc: Document) -> str:
    """ç”¨ LLM ä¸º chunk ç”Ÿæˆä¸Šä¸‹æ–‡æè¿°ï¼Œæ‹¼æ¥åé‡æ–°åµŒå…¥"""
    prompt = f"""
    <document_title>{doc.title}</document_title>
    <document_summary>{doc.doc_summary}</document_summary>
    <section_path>{chunk.section_path}</section_path>
    <chunk_content>{chunk.content}</chunk_content>
    
    è¯·ç”¨ 1-2 å¥è¯æè¿°è¿™ä¸ªæ–‡æœ¬å—åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­çš„ä½ç½®å’Œä½œç”¨ã€‚
    """
    context_desc = llm.generate(prompt)
    
    # å°†æè¿°æ‹¼åœ¨ chunk å‰é¢ï¼Œé‡æ–°ç”ŸæˆåµŒå…¥å‘é‡
    enriched_text = f"{context_desc}\n\n{chunk.content}"
    new_embedding = embed_model.encode(enriched_text)
    
    return new_embedding
```

**æ³¨æ„**ï¼šè¿™ä¸€æ­¥ä¼šå¢åŠ  Ingestion çš„æˆæœ¬ï¼ˆæ¯ä¸ª chunk è¦è°ƒä¸€æ¬¡ LLMï¼‰ï¼Œä½†æ£€ç´¢è´¨é‡æå‡æ˜¾è‘—ã€‚å¯ä»¥ç”¨è½»é‡çº§ LLMï¼ˆå¦‚ Claude Haikuï¼‰é™ä½æˆæœ¬ã€‚

**2.3.4 Query ç†è§£ä¸æ”¹å†™**

```python
class QueryRouter:
    def route(self, query: str, context: ConversationContext) -> QueryPlan:
        """åˆ†æç”¨æˆ·é—®é¢˜ï¼Œå†³å®šæ£€ç´¢ç­–ç•¥"""
        
        analysis = llm.generate(f"""
        åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œè¿”å› JSONï¼š
        é—®é¢˜ï¼š{query}
        
        è¿”å›æ ¼å¼ï¼š
        {{
          "query_type": "factual|summary|comparison|version_diff",
          "search_queries": ["æ”¹å†™åçš„æ£€ç´¢query1", "query2"],
          "metadata_filters": {{
            "doc_type": "åˆåŒ",      // å¦‚æœèƒ½è¯†åˆ«å‡º
            "department": null,
            "date_range": null,
            "specific_doc": null     // å¦‚æœæåˆ°äº†å…·ä½“æ–‡æ¡£å
          }},
          "needs_multi_doc": true/false,
          "estimated_scope": "narrow|medium|broad"
        }}
        """)
        
        return QueryPlan.from_json(analysis)
```

**2.3.5 è·¨æ–‡æ¡£ Map-Reduce æ€»ç»“**

```python
async def cross_document_summary(query: str, doc_ids: list[str]) -> str:
    """è·¨æ–‡æ¡£æ€»ç»“çš„æ ¸å¿ƒæµç¨‹"""
    
    # Step 1: æ£€ç´¢ç›¸å…³ chunksï¼ˆä»å¤šä¸ªæ–‡æ¡£ï¼‰
    relevant_chunks = retrieve(query, 
                                doc_ids=doc_ids, 
                                include_summaries=True,
                                top_k=30)
    
    # Step 2: æŒ‰æ–‡æ¡£åˆ†ç»„
    chunks_by_doc = group_by_document(relevant_chunks)
    
    # Step 3: Map â€”â€” å¯¹æ¯ä¸ªæ–‡æ¡£æå–ä¸é—®é¢˜ç›¸å…³çš„è¦ç‚¹
    doc_extracts = []
    for doc_id, chunks in chunks_by_doc.items():
        extract = await llm.generate(f"""
        ç”¨æˆ·é—®é¢˜ï¼š{query}
        
        ä»¥ä¸‹æ˜¯æ¥è‡ªã€Š{chunks[0].doc_title}ã€‹çš„ç›¸å…³å†…å®¹ï¼š
        {format_chunks(chunks)}
        
        è¯·æå–ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„è¦ç‚¹ï¼ˆ3-5 æ¡ï¼‰ï¼Œæ¯æ¡é™„å¸¦æ¥æºé¡µç ã€‚
        å¦‚æœè¯¥æ–‡æ¡£ä¸é—®é¢˜ä¸å¤ªç›¸å…³ï¼Œå›å¤"æ— ç›¸å…³å†…å®¹"ã€‚
        """)
        if "æ— ç›¸å…³å†…å®¹" not in extract:
            doc_extracts.append({
                "doc_title": chunks[0].doc_title,
                "doc_id": doc_id,
                "extract": extract
            })
    
    # Step 4: Reduce â€”â€” ç»¼åˆæ‰€æœ‰æ–‡æ¡£çš„è¦ç‚¹
    final_answer = await llm.generate(f"""
    ç”¨æˆ·é—®é¢˜ï¼š{query}
    
    ä»¥ä¸‹æ˜¯ä» {len(doc_extracts)} ä»½æ–‡æ¡£ä¸­æå–çš„ç›¸å…³è¦ç‚¹ï¼š
    
    {format_extracts(doc_extracts)}
    
    è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°çš„å›ç­”ï¼š
    1. å…ˆç»™å‡ºæ€»ä½“ç»“è®º/æ¦‚è¿°
    2. å†åˆ†ç‚¹å±•å¼€ç»†èŠ‚
    3. å¦‚æœä¸åŒæ–‡æ¡£å­˜åœ¨çŸ›ç›¾æˆ–å·®å¼‚ï¼Œæ˜ç¡®æŒ‡å‡º
    4. æ¯ä¸ªè®ºè¿°ç‚¹æ ‡æ³¨æ¥æºæ–‡æ¡£
    """)
    
    return final_answer
```

### 2.4 å‰ç«¯å¢å¼º

- æ–‡æ¡£ç®¡ç†ç•Œé¢ï¼šåˆ†ç»„ã€æ ‡ç­¾ã€ç­›é€‰
- é«˜çº§é—®ç­”ç•Œé¢ï¼š
  - å¯é€‰æ‹©é—®ç­”èŒƒå›´ï¼ˆå…¨éƒ¨æ–‡æ¡£ / æŒ‡å®šæ–‡æ¡£ç»„ / æŒ‡å®šæ–‡æ¡£ï¼‰
  - å¤šè½®å¯¹è¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
  - ç­”æ¡ˆä¸­çš„å¼•ç”¨å¯å±•å¼€æŸ¥çœ‹åŸæ–‡ç‰‡æ®µ
  - æ€»ç»“ç±»ç­”æ¡ˆæ˜¾ç¤º"æ¶‰åŠ N ä»½æ–‡æ¡£"åŠæ–‡æ¡£åˆ—è¡¨

### 2.5 éªŒè¯æ ‡å‡†

| æŒ‡æ ‡                      | ç›®æ ‡                | è¯´æ˜                             |
| ------------------------- | ------------------- | -------------------------------- |
| è·¨æ–‡æ¡£æ€»ç»“å‡†ç¡®ç‡          | â‰¥ 70%               | äººå·¥è¯„å®¡æ€»ç»“æ˜¯å¦æ¶µç›–æ‰€æœ‰å…³é”®ä¿¡æ¯ |
| è·¨æ–‡æ¡£æ€»ç»“å®Œæ•´åº¦          | â‰¥ 75%               | æ˜¯å¦é—æ¼äº†é‡è¦æ–‡æ¡£               |
| Contextual Retrieval æå‡ | Recall@5 æå‡ â‰¥ 10% | å¯¹æ¯” Phase 1 çš„ baseline         |
| Query æ”¹å†™æœ‰æ•ˆç‡          | â‰¥ 80%               | æ”¹å†™åçš„æ£€ç´¢ç»“æœä¼˜äºåŸå§‹ query   |
| æ€»ç»“ç±»æŸ¥è¯¢å“åº”æ—¶é—´        | < 30 ç§’             | æ¶‰åŠ 5-10 ä»½æ–‡æ¡£æ—¶               |

### 2.6 äº¤ä»˜ç‰©

- [x] æ–‡æ¡£åˆ†ç»„ç®¡ç†åŠŸèƒ½
- [x] é¢„å»ºæ‘˜è¦å±‚ï¼ˆç« èŠ‚ + æ–‡æ¡£çº§ï¼‰
- [x] Contextual Retrieval å®ç°
- [x] Query Router + Query æ”¹å†™
- [x] è·¨æ–‡æ¡£ Map-Reduce æ€»ç»“
- [x] å¤šè½®å¯¹è¯æ”¯æŒ
- [x] éªŒè¯æŠ¥å‘Š

---

## Phase 3ï¼šç‰ˆæœ¬ç®¡ç† & å·®å¼‚å¯¹æ¯”ï¼ˆç¬¬ 13-18 å‘¨ï¼‰

### 3.1 ç›®æ ‡

å®ç°æ–‡æ¡£å¤šç‰ˆæœ¬ç®¡ç†ã€ç‰ˆæœ¬è‡ªåŠ¨è¯†åˆ«ã€ä¸‰å±‚å·®å¼‚å¯¹æ¯”ã€‚
è§£å†³"åŒä¸€æ–‡æ¡£å¤šä¸ªç‰ˆæœ¬"å’Œ"ç‰ˆæœ¬é—´å·®å¼‚"çš„éœ€æ±‚ã€‚

### 3.2 æ•°æ®æ¨¡å‹æ‰©å±•

```sql
-- åœ¨ Phase 2 çš„ documents è¡¨åŸºç¡€ä¸Šæ–°å¢ç‰ˆæœ¬å­—æ®µ
ALTER TABLE documents ADD COLUMN version_number VARCHAR(20);
ALTER TABLE documents ADD COLUMN version_status VARCHAR(20) 
    DEFAULT 'active';  -- draft/active/superseded/archived
ALTER TABLE documents ADD COLUMN parent_version_id UUID 
    REFERENCES documents(doc_id);
ALTER TABLE documents ADD COLUMN is_latest BOOLEAN DEFAULT TRUE;
ALTER TABLE documents ADD COLUMN effective_date DATE;
ALTER TABLE documents ADD COLUMN superseded_at TIMESTAMPTZ;

-- ç‰ˆæœ¬å·®å¼‚è®°å½•è¡¨
CREATE TABLE version_diffs (
    diff_id          UUID PRIMARY KEY,
    old_version_id   UUID REFERENCES documents(doc_id),
    new_version_id   UUID REFERENCES documents(doc_id),
    diff_type        VARCHAR(20),   -- text/structural/semantic
    
    -- æ–‡æœ¬çº§å·®å¼‚
    text_diff_data   JSONB,         -- æ®µè½çº§åˆ«çš„å¢åˆ æ”¹è®°å½•
    
    -- ç»“æ„çº§å·®å¼‚
    structural_changes JSONB,       -- ç« èŠ‚å¢åˆ ã€é¡ºåºè°ƒæ•´
    
    -- è¯­ä¹‰çº§å·®å¼‚ï¼ˆLLM ç”Ÿæˆï¼‰
    change_summary   TEXT,          -- å˜æ›´æ¦‚è¿°
    change_details   JSONB,         -- åˆ†ç±»çš„å˜æ›´æ¡ç›®
    impact_analysis  TEXT,          -- å½±å“åˆ†æ
    
    created_at       TIMESTAMPTZ
);

-- ç‰ˆæœ¬é“¾ç´¢å¼•ï¼ˆåŠ é€Ÿç‰ˆæœ¬è¿½æº¯æŸ¥è¯¢ï¼‰
CREATE INDEX idx_version_chain 
    ON documents(parent_version_id) WHERE parent_version_id IS NOT NULL;
CREATE INDEX idx_latest_version 
    ON documents(doc_type, is_latest) WHERE is_latest = TRUE;
```

### 3.3 æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

**3.3.1 ç‰ˆæœ¬è‡ªåŠ¨è¯†åˆ«ï¼ˆå«æ–°æ—§åˆ¤æ–­ï¼‰**

```python
class VersionDetector:
    """åˆ¤æ–­æ–°ä¸Šä¼ çš„æ–‡æ¡£æ˜¯å¦ä¸ºå·²æœ‰æ–‡æ¡£çš„æ–°ç‰ˆæœ¬ï¼Œå¹¶åˆ¤æ–­è°æ›´æ–°"""
    
    AUTO_LINK_THRESHOLD = 0.8  # åªæœ‰ç½®ä¿¡åº¦é«˜äºæ­¤é˜ˆå€¼æ‰è‡ªåŠ¨å…³è”
    
    async def detect(self, new_doc_id, title, doc_summary, doc_type) -> VersionMatchResult:
        candidates = []
        
        # ç­–ç•¥ 1: æ ‡é¢˜ç›¸ä¼¼åº¦ (pg_trgm, similarity > 0.4)
        title_candidates = await self._find_by_title_similarity(
            title, new_doc_id, doc_type
        )
        candidates.extend(title_candidates)
        
        # ç­–ç•¥ 2: æ–‡æ¡£æ‘˜è¦å‘é‡ç›¸ä¼¼åº¦ (Qdrant, score > 0.75)
        if doc_summary:
            content_candidates = await self._find_by_content_similarity(
                doc_summary, new_doc_id
            )
            candidates.extend(content_candidates)
        
        if not candidates:
            return VersionMatchResult(is_new_version=False)
        
        # å»é‡åç”¨ LLM åšæœ€ç»ˆåˆ¤æ–­ï¼ˆåŒ…å«æ–°æ—§ç‰ˆæœ¬åˆ¤æ–­ï¼‰
        result = await self._llm_verify(title, doc_summary, unique_candidates)
        return result
    
    async def _llm_verify(self, new_title, new_summary, candidates) -> VersionMatchResult:
        """ç”¨ LLM æœ€ç»ˆç¡®è®¤æ˜¯å¦ä¸ºåŒä¸€æ–‡æ¡£çš„ä¸åŒç‰ˆæœ¬ï¼Œå¹¶åˆ¤æ–­è°æ›´æ–°"""
        prompt = f"""
        æ–°ä¸Šä¼ æ–‡æ¡£æ ‡é¢˜ï¼š{new_title}
        æ–°ä¸Šä¼ æ–‡æ¡£æ‘˜è¦ï¼š{new_summary[:500]}
        
        å€™é€‰å·²æœ‰æ–‡æ¡£ï¼š
        {format_candidates(candidates)}
        
        åˆ¤æ–­æ ‡å‡†ï¼š
        1. æ ‡é¢˜æ ¸å¿ƒéƒ¨åˆ†ç›¸åŒ + å†…å®¹ä¸»é¢˜ä¸€è‡´ â†’ åŒä¸€æ–‡æ¡£çš„ä¸åŒç‰ˆæœ¬
        2. åŒºåˆ†â€œåŒä¸€æ–‡æ¡£çš„ä¸åŒç‰ˆæœ¬â€å’Œâ€œåŒä¸€ç±»åˆ«ä½†ä¸åŒæ–‡æ¡£â€
        3. å¦‚æœæ˜¯åŒä¸€æ–‡æ¡£ï¼Œè¿›ä¸€æ­¥åˆ¤æ–­è°æ˜¯æ›´æ–°çš„ç‰ˆæœ¬ï¼Œä¾æ®åŒ…æ‹¬ï¼š
           - æ–‡æ¡£å†…éƒ¨çš„ç‰ˆæœ¬å·ï¼ˆv1.0ã€v2.0ã€ç¬¬Xç‰ˆç­‰ï¼‰
           - æ–‡æ¡£ä¸­çš„æ—¥æœŸï¼ˆç­¾ç½²/ç”Ÿæ•ˆ/ä¿®è®¢æ—¥æœŸï¼‰
           - å†…å®¹èŒƒå›´å˜åŒ–
        
        è¿”å› JSON:
        {{
          "is_new_version": true/false,
          "matched_doc_id": "...",
          "confidence": 0.95,
          "reason": "...",
          "new_is_newer": true/false,      // ä¸Šä¼ æ–‡æ¡£æ˜¯å¦ç¡®å®æ¯”å·²æœ‰æ–‡æ¡£æ›´æ–°
          "detected_version": "v2.0"       // ä»æ–‡æ¡£å†…å®¹ä¸­æå–çš„ç‰ˆæœ¬å·
        }}
        """
        result = await llm.generate_json(prompt)
        return VersionMatchResult(
            is_new_version=result["is_new_version"] and result["confidence"] >= 0.8,
            matched_doc_id=result.get("matched_doc_id"),
            confidence=result["confidence"],
            reason=result["reason"],
            new_is_newer=result.get("new_is_newer", True),
            detected_version=result.get("detected_version"),
        )
```

**3.3.2 ä¸‰å±‚å·®å¼‚å¯¹æ¯”å¼•æ“**

```python
class DiffEngine:
    """ä¸‰å±‚å·®å¼‚å¯¹æ¯”"""
    
    async def compute_diff(self, old_doc_id: str, new_doc_id: str) -> VersionDiff:
        old_doc = await load_parsed_document(old_doc_id)
        new_doc = await load_parsed_document(new_doc_id)
        
        # Layer 1: æ–‡æœ¬çº§å·®å¼‚ï¼ˆæ®µè½å¯¹é½ + diffï¼‰
        text_diff = self.compute_text_diff(old_doc, new_doc)
        
        # Layer 2: ç»“æ„çº§å·®å¼‚ï¼ˆç« èŠ‚å¢åˆ æ”¹ï¼‰
        structural_diff = self.compute_structural_diff(old_doc, new_doc)
        
        # Layer 3: è¯­ä¹‰çº§å·®å¼‚ï¼ˆLLM åˆ†æï¼‰
        semantic_diff = await self.compute_semantic_diff(
            old_doc, new_doc, text_diff, structural_diff
        )
        
        return VersionDiff(
            text_diff=text_diff,
            structural_diff=structural_diff,
            semantic_diff=semantic_diff
        )
    
    def compute_text_diff(self, old_doc, new_doc) -> TextDiff:
        """æ®µè½çº§åˆ«çš„æ–‡æœ¬å¯¹æ¯”"""
        # å°†ä¸¤ä¸ªæ–‡æ¡£æŒ‰ç« èŠ‚å¯¹é½
        aligned_sections = self.align_sections(
            old_doc.sections, new_doc.sections
        )
        
        diffs = []
        for old_section, new_section in aligned_sections:
            if old_section is None:
                diffs.append(SectionDiff(type="added", 
                                         new=new_section))
            elif new_section is None:
                diffs.append(SectionDiff(type="deleted", 
                                         old=old_section))
            else:
                # æ®µè½çº§ diff
                para_diffs = difflib.unified_diff(
                    old_section.paragraphs,
                    new_section.paragraphs,
                    lineterm=""
                )
                if para_diffs:
                    diffs.append(SectionDiff(
                        type="modified",
                        old=old_section,
                        new=new_section,
                        paragraph_diffs=para_diffs
                    ))
        return TextDiff(section_diffs=diffs)
    
    def compute_structural_diff(self, old_doc, new_doc) -> StructuralDiff:
        """ç« èŠ‚ç»“æ„å¯¹æ¯”"""
        old_toc = extract_toc(old_doc)   # æå–ç›®å½•ç»“æ„
        new_toc = extract_toc(new_doc)
        
        return StructuralDiff(
            added_sections=[s for s in new_toc if s not in old_toc],
            deleted_sections=[s for s in old_toc if s not in new_toc],
            reordered_sections=detect_reordering(old_toc, new_toc),
            renamed_sections=detect_renames(old_toc, new_toc)
        )
    
    async def compute_semantic_diff(self, old_doc, new_doc, 
                                     text_diff, structural_diff) -> SemanticDiff:
        """LLM åˆ†æå˜æ›´çš„ä¸šåŠ¡å«ä¹‰"""
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªç‰ˆæœ¬æ–‡æ¡£ä¹‹é—´çš„å˜æ›´ã€‚

        æ–‡æ¡£æ ‡é¢˜ï¼š{new_doc.title}
        æ—§ç‰ˆæœ¬ï¼š{old_doc.version_number}
        æ–°ç‰ˆæœ¬ï¼š{new_doc.version_number}

        ç»“æ„å˜åŒ–ï¼š
        - æ–°å¢ç« èŠ‚ï¼š{structural_diff.added_sections}
        - åˆ é™¤ç« èŠ‚ï¼š{structural_diff.deleted_sections}

        ä¸»è¦æ–‡æœ¬å˜åŒ–ï¼ˆæ‘˜è¦ï¼‰ï¼š
        {summarize_text_diff(text_diff, max_length=2000)}

        è¯·è¿”å› JSON æ ¼å¼çš„åˆ†æï¼š
        {{
          "change_summary": "ä¸€æ®µè¯æ¦‚è¿°ä¸»è¦å˜æ›´",
          "changes": [
            {{
              "category": "å®è´¨æ€§å˜æ›´|æªè¾è°ƒæ•´|æ ¼å¼å˜æ›´|æ–°å¢å†…å®¹|åˆ é™¤å†…å®¹",
              "description": "å…·ä½“å˜æ›´æè¿°",
              "location": "æ¶‰åŠçš„ç« èŠ‚",
              "old_text_snippet": "æ—§ç‰ˆåŸæ–‡å…³é”®ç‰‡æ®µ",
              "new_text_snippet": "æ–°ç‰ˆåŸæ–‡å…³é”®ç‰‡æ®µ",
              "business_impact": "å¯¹ä¸šåŠ¡çš„æ½œåœ¨å½±å“"
            }}
          ],
          "risk_flags": ["éœ€è¦æ³¨æ„çš„é«˜é£é™©å˜æ›´"],
          "overall_impact": "æ€»ä½“å½±å“è¯„ä¼°"
        }}
        """
        return llm.generate_json(prompt)
```

**3.3.3 ç‰ˆæœ¬æ„ŸçŸ¥æ£€ç´¢**

```python
class VersionAwareRetriever:
    """æ ¹æ®ç”¨æˆ·æ„å›¾å†³å®šç‰ˆæœ¬æ£€ç´¢ç­–ç•¥"""
    
    async def retrieve(self, query: str, query_plan: QueryPlan) -> list[Chunk]:
        
        if query_plan.query_type == "version_diff":
            # ç‰ˆæœ¬å¯¹æ¯”æ¨¡å¼ï¼šè·å–ä¸¤ä¸ªç‰ˆæœ¬çš„å†…å®¹
            return await self.retrieve_for_comparison(query, query_plan)
        
        elif query_plan.query_type == "version_history":
            # ç‰ˆæœ¬è¿½æº¯æ¨¡å¼ï¼šè·å–æ‰€æœ‰ç‰ˆæœ¬çš„ç›¸å…³å†…å®¹
            return await self.retrieve_all_versions(query, query_plan)
        
        else:
            # é»˜è®¤æ¨¡å¼ï¼šåªæ£€ç´¢æœ€æ–°ç‰ˆæœ¬
            filters = {
                **query_plan.metadata_filters,
                "is_latest": True      # â† å…³é”®ï¼šé»˜è®¤åªçœ‹æœ€æ–°ç‰ˆ
            }
            return await self.standard_retrieve(query, filters)
    
    async def retrieve_for_comparison(self, query, query_plan):
        """ç‰ˆæœ¬å¯¹æ¯”æ£€ç´¢"""
        old_version, new_version = identify_versions(query, query_plan)
        
        # è·å–é¢„è®¡ç®—çš„ diff è®°å½•
        diff = await db.get_version_diff(old_version.doc_id, 
                                          new_version.doc_id)
        
        if diff:
            return diff  # è¿”å›å·²æœ‰çš„å¯¹æ¯”ç»“æœ
        else:
            # è§¦å‘å®æ—¶è®¡ç®—
            diff = await diff_engine.compute_diff(
                old_version.doc_id, new_version.doc_id
            )
            await db.save_version_diff(diff)
            return diff
```

### 3.4 ç‰ˆæœ¬å…¥åº“å®Œæ•´æµç¨‹

```
æ–°æ–‡æ¡£ä¸Šä¼ 
    â”‚
    â–¼
[Phase 1: è§£æ + åˆ†å—]
    â”‚
    â–¼
[Phase 2: ç”Ÿæˆæ‘˜è¦ + å®ä½“æå–]
    â”‚
    â–¼
[ç‰ˆæœ¬æ£€æµ‹] â”€â”€â†’ æ˜¯å¦ä¸ºå·²æœ‰æ–‡æ¡£çš„ä¸åŒç‰ˆæœ¬ï¼Ÿ
    â”‚              â”‚
    â”‚       æ˜¯     â”‚      å¦
    â”‚       â–¼      â”‚      â–¼
    â”‚  [åˆ¤æ–­æ–°æ—§]   â”‚  [ä½œä¸ºå…¨æ–°æ–‡æ¡£å…¥åº“]
    â”‚       â”‚      â”‚  â”‚  å…¨éƒ¨ chunks is_latest=TRUE
    â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚  â”‚          â”‚
    â”‚  â–¼          â–¼
    â”‚ ä¸Šä¼ æ›´æ–°    ä¸Šä¼ æ›´æ—§
    â”‚ (new_is_newer=true)  (new_is_newer=false)
    â”‚  â”‚          â”‚
    â”‚  â–¼          â–¼
    â”‚ [_link_version]     [_link_as_older_version]
    â”‚  - æ–°æ–‡æ¡£.parent = æ—§æ–‡æ¡£    - æ–°æ–‡æ¡£æ’å…¥ä¸ºå·²æœ‰æ–‡æ¡£çš„çˆ¶ç‰ˆæœ¬
    â”‚  - æ–°æ–‡æ¡£ is_latest=TRUE     - æ–°æ–‡æ¡£ is_latest=FALSE
    â”‚  - æ—§æ–‡æ¡£ is_latest=FALSE    - å·²æœ‰æ–‡æ¡£ä¿æŒ is_latest=TRUE
    â”‚  - æ—§æ–‡æ¡£ status=superseded  - æ–°æ–‡æ¡£ status=superseded
    â”‚  â”‚          â”‚
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚       â–¼
    â”‚  [è§¦å‘å·®å¼‚è®¡ç®—ï¼ˆå¼‚æ­¥ï¼‰]
    â”‚  - æ–‡æœ¬çº§ diff
    â”‚  - ç»“æ„çº§ diff
    â”‚  - è¯­ä¹‰çº§ diff (LLM)
    â”‚  - å­˜å…¥ version_diffs è¡¨
    â”‚       â”‚
    â–¼       â–¼
[ä¸Šä¸‹æ–‡å¢å¼ºåµŒå…¥ + å­˜å‚¨]
    â”‚  chunks ä½¿ç”¨ is_doc_latest å†³å®š is_latest æ ‡è®°
    â–¼
[å®Œæˆ] ç”¨æˆ·å¯æŸ¥è¯¢æœ€æ–°ç‰ˆæœ¬ã€å¯¹æ¯”ç‰ˆæœ¬ã€è¿½æº¯å†å²
```

### 3.5 å‰ç«¯æ–°å¢

- ç‰ˆæœ¬æ—¶é—´çº¿è§†å›¾ï¼ˆç‚¹å‡»ä»»ä¸€ç‰ˆæœ¬å¯æŸ¥çœ‹è¯¦æƒ…ï¼‰
- åŒæ å¯¹æ¯”è§†å›¾ï¼ˆçº¢ç»¿æ ‡æ³¨å·®å¼‚ï¼Œç±»ä¼¼ Git diffï¼‰
- è¯­ä¹‰å˜æ›´æ‘˜è¦å¡ç‰‡ï¼ˆåˆ†ç±»å±•ç¤ºï¼šå®è´¨æ€§å˜æ›´ / æªè¾è°ƒæ•´ / æ ¼å¼å˜æ›´ï¼‰
- ç‰ˆæœ¬ä¸Šä¼ æ—¶çš„"è¯†åˆ«ä¸ºæ–°ç‰ˆæœ¬"ç¡®è®¤å¼¹çª—

### 3.6 éªŒè¯æ ‡å‡†

| æŒ‡æ ‡               | ç›®æ ‡                    |
| ------------------ | ----------------------- |
| ç‰ˆæœ¬è‡ªåŠ¨è¯†åˆ«å‡†ç¡®ç‡ | â‰¥ 90%                   |
| æ–‡æœ¬çº§ diff å‡†ç¡®ç‡ | â‰¥ 95%                   |
| è¯­ä¹‰å˜æ›´æ‘˜è¦è´¨é‡   | äººå·¥è¯„å®¡ â‰¥ 80% æ»¡æ„åº¦   |
| ç‰ˆæœ¬æ£€ç´¢æ­£ç¡®æ€§     | é»˜è®¤æŸ¥è¯¢è¿”å›æœ€æ–°ç‰ˆ 100% |
| diff è®¡ç®—æ—¶é—´      | < 2 åˆ†é’Ÿ / 50 é¡µæ–‡æ¡£å¯¹  |

### 3.7 äº¤ä»˜ç‰©

- [x] ç‰ˆæœ¬è‡ªåŠ¨è¯†åˆ«æ¨¡å—
- [x] ä¸‰å±‚å·®å¼‚å¯¹æ¯”å¼•æ“
- [x] ç‰ˆæœ¬æ„ŸçŸ¥æ£€ç´¢
- [x] ç‰ˆæœ¬ç®¡ç† UIï¼ˆæ—¶é—´çº¿ + å¯¹æ¯”è§†å›¾ï¼‰
- [x] éªŒè¯æŠ¥å‘Š

---

## Phase 4ï¼šæ™ºèƒ½ç¼–æ’ & Agent åŒ–ï¼ˆç¬¬ 19-24 å‘¨ï¼‰

### 4.1 ç›®æ ‡

å¼•å…¥ Agent æ¨¡å¼ï¼Œè®©ç³»ç»Ÿèƒ½è‡ªä¸»è§„åˆ’å¤æ‚æŸ¥è¯¢çš„å¤„ç†æ­¥éª¤ã€‚
å°† Phase 1-3 çš„æ‰€æœ‰èƒ½åŠ›ç¼–æ’æˆå¯ç»„åˆçš„å·¥å…·é›†ã€‚

### 4.2 Agent å·¥å…·é›†å®šä¹‰

```python
# Agent å¯è°ƒç”¨çš„å·¥å…·
TOOLS = [
    Tool(
        name="search_documents",
        description="åœ¨æ–‡æ¡£åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹ã€‚æ”¯æŒè¯­ä¹‰æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢ã€‚",
        params=["query", "doc_type_filter", "group_filter", 
                "date_range", "version_filter", "top_k"]
    ),
    Tool(
        name="read_document_summary",
        description="è¯»å–æŒ‡å®šæ–‡æ¡£çš„æ•´ä½“æ‘˜è¦æˆ–æŒ‡å®šç« èŠ‚çš„æ‘˜è¦ã€‚",
        params=["doc_id", "section_path"]  # section_path ä¸ºç©ºåˆ™è¿”å›æ–‡æ¡£æ‘˜è¦
    ),
    Tool(
        name="read_document_detail",
        description="è¯»å–æŒ‡å®šæ–‡æ¡£çš„ç‰¹å®šç« èŠ‚æˆ–é¡µç èŒƒå›´çš„è¯¦ç»†å†…å®¹ã€‚",
        params=["doc_id", "section_path", "page_range"]
    ),
    Tool(
        name="list_documents",
        description="åˆ—å‡ºç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£æ¸…å•ã€‚",
        params=["doc_type", "group", "tags", "date_range", "status"]
    ),
    Tool(
        name="compare_versions",
        description="å¯¹æ¯”åŒä¸€æ–‡æ¡£çš„ä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´çš„å·®å¼‚ã€‚",
        params=["doc_id", "old_version", "new_version"]
    ),
    Tool(
        name="get_version_history",
        description="è·å–æŸä»½æ–‡æ¡£çš„ç‰ˆæœ¬å†å²è®°å½•ã€‚",
        params=["doc_id"]
    ),
    Tool(
        name="cross_document_analysis",
        description="å¯¹å¤šä»½æ–‡æ¡£çš„æŒ‡å®šä¸»é¢˜è¿›è¡Œè·¨æ–‡æ¡£å¯¹æ¯”åˆ†æã€‚",
        params=["doc_ids", "analysis_topic", "analysis_type"]
        # analysis_type: comparison | summary | extract_common | find_differences
    ),
    Tool(
        name="generate_report",
        description="åŸºäºå·²æ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Šã€‚",
        params=["collected_info", "report_format", "audience"]
    ),
]
```

### 4.3 Agent æ ¸å¿ƒé€»è¾‘

```python
class DocumentAgent:
    """æ–‡æ¡£åˆ†æ Agentï¼Œå¯è‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œå¤æ‚æŸ¥è¯¢"""
    
    SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€ä¸ªä¼ä¸šæ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
    {tools_description}
    
    å¤„ç†é—®é¢˜çš„åŸåˆ™ï¼š
    1. å…ˆç†è§£ç”¨æˆ·é—®é¢˜çš„ç±»å‹å’ŒèŒƒå›´
    2. å¯¹äºç®€å•äº‹å®æ€§é—®é¢˜ï¼Œç›´æ¥æ£€ç´¢åå›ç­”
    3. å¯¹äºæ€»ç»“æ€§é—®é¢˜ï¼Œå…ˆçœ‹æ–‡æ¡£æ‘˜è¦äº†è§£å…¨å±€ï¼Œå†æ·±å…¥ç»†èŠ‚
    4. å¯¹äºå¯¹æ¯”ç±»é—®é¢˜ï¼Œåˆ†åˆ«è·å–å„æ–¹ä¿¡æ¯åç»¼åˆåˆ†æ
    5. å¯¹äºç‰ˆæœ¬ç›¸å…³é—®é¢˜ï¼Œå…ˆæŸ¥ç‰ˆæœ¬å†å²ï¼Œå†åšé’ˆå¯¹æ€§å¯¹æ¯”
    6. æ¯æ­¥æ“ä½œåè¯„ä¼°æ˜¯å¦å·²æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”é—®é¢˜
    7. ç­”æ¡ˆå¿…é¡»æ ‡æ³¨ä¿¡æ¯æ¥æº
    
    æ€è€ƒæ­¥éª¤æ ¼å¼ï¼š
    Thought: åˆ†æå½“å‰æƒ…å†µï¼Œå†³å®šä¸‹ä¸€æ­¥æ“ä½œ
    Action: è°ƒç”¨å·¥å…·å
    Action Input: å·¥å…·å‚æ•°
    Observation: å·¥å…·è¿”å›ç»“æœ
    ... (é‡å¤ç›´åˆ°æœ‰è¶³å¤Ÿä¿¡æ¯)
    Final Answer: æœ€ç»ˆç­”æ¡ˆ
    """
    
    async def run(self, query: str, context: ConversationContext) -> AgentResponse:
        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            *context.history,
            {"role": "user", "content": query}
        ]
        
        max_steps = 8  # é˜²æ­¢æ— é™å¾ªç¯
        steps = []
        
        for step in range(max_steps):
            response = await llm.generate(messages, tools=TOOLS)
            
            if response.has_tool_call:
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_result = await self.execute_tool(
                    response.tool_name, response.tool_params
                )
                steps.append(AgentStep(
                    thought=response.thought,
                    action=response.tool_name,
                    observation=tool_result
                ))
                messages.append({"role": "assistant", "content": response.raw})
                messages.append({"role": "tool", "content": str(tool_result)})
            else:
                # Agent è®¤ä¸ºä¿¡æ¯è¶³å¤Ÿï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                return AgentResponse(
                    answer=response.final_answer,
                    steps=steps,      # å¯åœ¨å‰ç«¯å±•ç¤ºæ¨ç†è¿‡ç¨‹
                    sources=extract_sources(steps)
                )
        
        # è¶…è¿‡æœ€å¤§æ­¥éª¤ï¼Œå¼ºåˆ¶ç”Ÿæˆç­”æ¡ˆ
        return self.force_final_answer(messages, steps)
```

### 4.4 Query Router å‡çº§

```python
class SmartQueryRouter:
    """æ™ºèƒ½è·¯ç”±ï¼šç®€å•é—®é¢˜èµ°å¿«é€Ÿé€šé“ï¼Œå¤æ‚é—®é¢˜èµ° Agent"""
    
    async def route(self, query: str, context) -> Response:
        intent = await self.classify_intent(query)
        
        match intent:
            case "simple_factual":
                # å¿«é€Ÿé€šé“ï¼šç›´æ¥ RAGï¼Œä¸èµ° Agent
                return await simple_rag_pipeline(query)
            
            case "single_doc_query":
                # å•æ–‡æ¡£é—®ç­”ï¼šå¸¦ä¸Šä¸‹æ–‡æ‰©å±•çš„ RAG
                return await enhanced_rag_pipeline(query)
            
            case "cross_doc_summary":
                # è·¨æ–‡æ¡£æ€»ç»“ï¼šèµ° Agent
                return await document_agent.run(query, context)
            
            case "version_comparison":
                # ç‰ˆæœ¬å¯¹æ¯”ï¼šèµ°ä¸“ç”¨ workflow
                return await version_comparison_workflow(query, context)
            
            case "complex_analysis":
                # å¤æ‚åˆ†æï¼šèµ° Agent
                return await document_agent.run(query, context)
            
            case _:
                # å…œåº•ï¼šèµ° Agent
                return await document_agent.run(query, context)
```

### 4.5 å‰ç«¯å¢å¼º

- Agent æ¨ç†è¿‡ç¨‹å¯è§†åŒ–ï¼ˆå±•ç¤º"æ­£åœ¨æœç´¢..." â†’ "æ­£åœ¨åˆ†æ..." â†’ "ç”Ÿæˆç­”æ¡ˆ..."ï¼‰
- ç”¨æˆ·å¯å¹²é¢„ Agent æ­¥éª¤ï¼ˆå¦‚ï¼š"ä¸ç”¨çœ‹è¿™ä»½æ–‡æ¡£ï¼Œçœ‹é‚£ä»½"ï¼‰
- å¤æ‚æŸ¥è¯¢çš„è¿›åº¦æŒ‡ç¤ºå™¨
- æ”¶è—/åˆ†äº«åˆ†æç»“æœ

### 4.6 éªŒè¯æ ‡å‡†

| æŒ‡æ ‡             | ç›®æ ‡                          |
| ---------------- | ----------------------------- |
| Agent è·¯ç”±å‡†ç¡®ç‡ | â‰¥ 90%ï¼ˆæ­£ç¡®é€‰æ‹©å¤„ç†é€šé“ï¼‰     |
| å¤æ‚æŸ¥è¯¢æˆåŠŸç‡   | â‰¥ 75%ï¼ˆAgent èƒ½ç»™å‡ºæœ‰ç”¨ç­”æ¡ˆï¼‰ |
| Agent å¹³å‡æ­¥éª¤æ•° | â‰¤ 5 æ­¥ï¼ˆæ•ˆç‡ï¼‰                |
| Agent è¶…æ—¶ç‡     | < 10%ï¼ˆ8 æ­¥å†…å®Œæˆï¼‰           |
| ç”¨æˆ·æ»¡æ„åº¦       | â‰¥ 80%                         |

### 4.7 äº¤ä»˜ç‰©

- [x] Agent æ¡†æ¶ + å·¥å…·é›†
- [x] æ™ºèƒ½ Query Router
- [x] æ¨ç†è¿‡ç¨‹å¯è§†åŒ– UI
- [x] éªŒè¯æŠ¥å‘Š

---

## Phase 5ï¼šç”Ÿäº§åŒ–åŠ å›º & æŒç»­ä¼˜åŒ–ï¼ˆç¬¬ 25-30 å‘¨ï¼‰

### 5.1 ç›®æ ‡

å°†ç³»ç»Ÿä»"èƒ½ç”¨"æå‡åˆ°"å¥½ç”¨ + å¯é  + å®‰å…¨"ã€‚

### 5.2 å·¥ä½œæ¸…å•

**5.2.1 æƒé™ä¸å®‰å…¨**

```python
# æ£€ç´¢æ—¶çš„æƒé™è¿‡æ»¤ï¼ˆåœ¨å‘é‡æ£€ç´¢å±‚å®ç°ï¼‰
async def search_with_permissions(query, user: User):
    # è·å–ç”¨æˆ·å¯è®¿é—®çš„æ–‡æ¡£åˆ—è¡¨
    accessible_docs = await get_user_accessible_docs(user.id)
    
    # åœ¨å‘é‡æ£€ç´¢æ—¶å°±åšæƒé™è¿‡æ»¤ï¼ˆè€Œéæ£€ç´¢åè¿‡æ»¤ï¼‰
    results = await qdrant.search(
        query_vector=embed(query),
        query_filter=Filter(
            must=[
                FieldCondition(key="doc_id", 
                               match=MatchAny(any=accessible_docs))
            ]
        )
    )
    return results
```

æ ¸å¿ƒå®‰å…¨æªæ–½æ¸…å•ï¼š
- æ–‡æ¡£çº§ RBAC æƒé™æ§åˆ¶
- æ£€ç´¢å±‚æƒé™è¿‡æ»¤ï¼ˆéç”Ÿæˆå±‚è¿‡æ»¤ï¼‰
- å®¡è®¡æ—¥å¿—ï¼ˆè°åœ¨ä»€ä¹ˆæ—¶é—´æŸ¥äº†ä»€ä¹ˆï¼Œå¾—åˆ°äº†ä»€ä¹ˆç»“æœï¼‰
- æ•°æ®åŠ å¯†ï¼ˆä¼ è¾“åŠ å¯† TLS + å­˜å‚¨åŠ å¯† AES-256ï¼‰
- æ•æ„Ÿæ–‡æ¡£éš”ç¦»å­˜å‚¨
- LLM è¾“å‡ºå®‰å…¨å®¡æŸ¥ï¼ˆé˜²æ­¢æ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼‰

**5.2.2 æ€§èƒ½ä¼˜åŒ–**

- åµŒå…¥è®¡ç®—æ‰¹å¤„ç† + GPU åŠ é€Ÿ
- çƒ­é—¨æŸ¥è¯¢ç¼“å­˜ï¼ˆRedisï¼Œç‰ˆæœ¬æ›´æ–°æ—¶è‡ªåŠ¨å¤±æ•ˆï¼‰
- æ–‡æ¡£å¤„ç†å¼‚æ­¥é˜Ÿåˆ—ï¼ˆä¸Šä¼ å³è¿”å›ï¼Œåå°å¤„ç†ï¼‰
- LLM è°ƒç”¨æµå¼è¾“å‡ºï¼ˆStreaming SSEï¼‰
- å¤§æ–‡æ¡£åˆ†ç‰‡å¹¶è¡Œå¤„ç†

**5.2.3 å¯è§‚æµ‹æ€§**

```yaml
# ç›‘æ§æŒ‡æ ‡
metrics:
  - document_processing_duration_seconds    # æ–‡æ¡£å¤„ç†è€—æ—¶
  - retrieval_latency_seconds               # æ£€ç´¢å»¶è¿Ÿ
  - llm_call_duration_seconds               # LLM è°ƒç”¨è€—æ—¶
  - llm_token_usage_total                   # Token æ¶ˆè€—
  - query_success_rate                      # æŸ¥è¯¢æˆåŠŸç‡
  - agent_step_count                        # Agent æ­¥éª¤æ•°
  - cache_hit_rate                          # ç¼“å­˜å‘½ä¸­ç‡

# å‘Šè­¦è§„åˆ™
alerts:
  - query_latency > 30s for 5 minutes       # æŸ¥è¯¢å»¶è¿Ÿå‘Šè­¦
  - llm_error_rate > 5% for 10 minutes      # LLM é”™è¯¯ç‡å‘Šè­¦
  - document_processing_queue > 100         # å¤„ç†é˜Ÿåˆ—å †ç§¯
```

**5.2.4 æŒç»­è¯„ä¼° Pipeline**

```python
class EvaluationPipeline:
    """å®šæœŸè‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿè´¨é‡"""
    
    async def run_weekly_eval(self):
        test_set = load_test_questions()   # æ ‡æ³¨å¥½çš„æµ‹è¯•é›†
        
        results = {
            "retrieval_recall": [],
            "answer_accuracy": [],
            "citation_accuracy": [],
            "latency": []
        }
        
        for question in test_set:
            response = await system.query(question.text)
            
            # è‡ªåŠ¨è¯„ä¼°ï¼ˆç”¨ LLM ä½œä¸ºè¯„åˆ¤ï¼‰
            eval_result = await llm_judge.evaluate(
                question=question.text,
                expected_answer=question.ground_truth,
                actual_answer=response.answer,
                retrieved_chunks=response.sources,
                expected_sources=question.expected_sources
            )
            
            results["retrieval_recall"].append(eval_result.recall)
            results["answer_accuracy"].append(eval_result.accuracy)
            # ...
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š + è¶‹åŠ¿å¯¹æ¯”
        report = generate_eval_report(results, previous_results)
        await notify_team(report)
```

**5.2.5 ç”¨æˆ·åé¦ˆé—­ç¯**

- æ¯ä¸ªå›ç­”æ—è¾¹çš„ ğŸ‘ğŸ‘ åé¦ˆæŒ‰é’®
- ğŸ‘ åé¦ˆè‡ªåŠ¨æ”¶é›†åˆ°è¯„ä¼°é˜Ÿåˆ—
- å®šæœŸåˆ†æ bad caseï¼Œé’ˆå¯¹æ€§ä¼˜åŒ–
- åé¦ˆæ•°æ®åå“º Query Router å’Œæ£€ç´¢ç­–ç•¥çš„è¿­ä»£

### 5.3 äº¤ä»˜ç‰©

- [x] æƒé™ç³»ç»Ÿ (RBAC)
- [x] å®¡è®¡æ—¥å¿—ç³»ç»Ÿ
- [x] æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜ã€å¼‚æ­¥ã€æµå¼ï¼‰
- [x] ç›‘æ§å‘Šè­¦ Dashboard
- [x] è‡ªåŠ¨è¯„ä¼° Pipeline
- [x] ç”¨æˆ·åé¦ˆé—­ç¯
- [x] è¿ç»´æ‰‹å†Œ + API æ–‡æ¡£

---

## é™„å½• Aï¼šæˆæœ¬ä¼°ç®—å‚è€ƒ

| é¡¹ç›®                       | Phase 1 æœˆæˆæœ¬ | Phase 5 æœˆæˆæœ¬ | è¯´æ˜                        |
| -------------------------- | -------------- | -------------- | --------------------------- |
| LLM API (Claude Sonnet)    | ~$200          | ~$2,000        | æŒ‰ 100 ç”¨æˆ·æ—¥å‡ 20 æŸ¥è¯¢ä¼°ç®— |
| GPU æœåŠ¡å™¨ (åµŒå…¥+Reranker) | ~$500          | ~$500          | 1 å¼  A10/L4 è¶³å¤Ÿ            |
| äº‘æœåŠ¡å™¨ (åº”ç”¨+æ•°æ®åº“)     | ~$300          | ~$800          | æ ¹æ®ç”¨æˆ·é‡å¼¹æ€§æ‰©å±•          |
| å¯¹è±¡å­˜å‚¨                   | ~$10           | ~$50           | æŒ‰ 1TB æ–‡æ¡£ä¼°ç®—             |
| **æœˆæ€»è®¡**                 | **~$1,010**    | **~$3,350**    |                             |

> æ³¨ï¼šå¦‚é€‰æ‹©ç§æœ‰åŒ–éƒ¨ç½² LLMï¼ˆå¦‚ Qwen2.5-72Bï¼‰ï¼Œéœ€é¢å¤– GPU æˆæœ¬ä½†å¯æ¶ˆé™¤ API è´¹ç”¨ã€‚

## é™„å½• Bï¼šå›¢é˜Ÿé…ç½®å»ºè®®

| Phase     | æœ€å°å›¢é˜Ÿ               | å»ºè®®å›¢é˜Ÿ                              |
| --------- | ---------------------- | ------------------------------------- |
| Phase 0-1 | 1 å…¨æ ˆ + 1 ML/NLP      | 2 åç«¯ + 1 å‰ç«¯ + 1 ML                |
| Phase 2-3 | 2 åç«¯ + 1 ML          | 3 åç«¯ + 1 å‰ç«¯ + 1 ML + 0.5 PM       |
| Phase 4-5 | 2 åç«¯ + 1 ML + 1 å‰ç«¯ | 3 åç«¯ + 2 å‰ç«¯ + 1 ML + 1 SRE + 1 PM |

## é™„å½• Cï¼šå…³é”®é£é™©ä¸åº”å¯¹

| é£é™©                   | å½±å“     | åº”å¯¹ç­–ç•¥                             |
| ---------------------- | -------- | ------------------------------------ |
| LLM API ä¸ç¨³å®š/é™æµ    | æŸ¥è¯¢å¤±è´¥ | å¤š LLM å¤‡é€‰ + é™çº§ç­–ç•¥               |
| æ–‡æ¡£è§£æè´¨é‡å·®         | æ£€ç´¢ä¸å‡† | å¤šå¼•æ“ç»„åˆ + äººå·¥æŠ½æ£€                |
| å‘é‡åº“æ•°æ®é‡å¢é•¿è¶…é¢„æœŸ | æ€§èƒ½ä¸‹é™ | åˆ†ç‰‡ç­–ç•¥ + å®šæœŸæ¸…ç†æ—§ç‰ˆæœ¬ç´¢å¼•        |
| è·¨æ–‡æ¡£æ€»ç»“å¹»è§‰         | ç”¨æˆ·è¯¯å¯¼ | å¼ºåˆ¶å¼•ç”¨ + ç½®ä¿¡åº¦è¯„åˆ† + äººå·¥è¯„å®¡     |
| ç‰ˆæœ¬è¯†åˆ«è¯¯åˆ¤           | ç‰ˆæœ¬æ··ä¹± | é«˜é˜ˆå€¼ + äººå·¥ç¡®è®¤å…œåº•                |
| LLM æˆæœ¬è¶…é¢„æœŸ         | é¢„ç®—å‹åŠ› | ç¼“å­˜ + å°æ¨¡å‹åšè½»é‡ä»»åŠ¡ + Token ç›‘æ§ |
